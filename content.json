{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"Huayan Wong","url":"http://yoursite.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2019-04-03T07:50:09.926Z","updated":"2019-04-03T07:29:05.746Z","comments":false,"path":"/404.html","permalink":"http://yoursite.com//404.html","excerpt":"","text":""},{"title":"关于","date":"2019-04-03T07:50:09.969Z","updated":"2019-04-03T07:29:05.752Z","comments":false,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2019-04-03T07:50:09.979Z","updated":"2019-04-03T07:29:05.753Z","comments":false,"path":"books/index.html","permalink":"http://yoursite.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-04-03T07:50:10.001Z","updated":"2019-04-03T07:29:05.755Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-04-03T07:50:09.989Z","updated":"2019-04-03T07:29:05.757Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-04-03T08:06:44.000Z","updated":"2019-04-08T10:16:32.049Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-04-03T07:50:10.011Z","updated":"2019-04-03T07:29:05.760Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring Boot日志","slug":"2018-7-5-SpringBoot之日志","date":"2019-04-15T03:46:00.292Z","updated":"2019-04-15T03:49:23.139Z","comments":true,"path":"2019/04/15/2018-7-5-SpringBoot之日志/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-7-5-SpringBoot之日志/","excerpt":"","text":"","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"}],"author":"wonghy"},{"title":"Spring Boot之注解类","slug":"2018-7-2-SpringBoot之注解类","date":"2019-04-15T03:46:00.286Z","updated":"2019-04-15T03:49:23.093Z","comments":true,"path":"2019/04/15/2018-7-2-SpringBoot之注解类/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-7-2-SpringBoot之注解类/","excerpt":"","text":"1、@ConfigurationProperties和@Value的区别123456789101112131415例子：@Component//@ConfigurationProperties(prefix = \"author\")public class User &#123; private Integer id; private String name; private Integer age; /* &lt;bean class=\"User\"&gt; &lt;property name=\"name\" value=\"字面量/$&#123;key&#125;从环境变量、配置文件中获取/#&#123;spEL&#125;\"&gt;&lt;/property&gt; &lt;/bean&gt;*/@Value用法：等同于上面的中的value赋值区别@ConfigurationProperties@Value功能批量注入配置文件中的属性一个个指定松散绑定（松散语法）支持不支持SpEL不支持支持JSR303数据校验（@Email等）支持不支持如果说，只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value如果说，专门编写一个javaBean来和配置文件进行映射，使用@ConfigurationProperties2、@PropertySource&amp;@ImportSource@PropertySource:加载指定的配置文件123456789@Component//@ConfigurationProperties(prefix = \"author\")@PropertySource(value = &#123;\"classpath:person.properties\"&#125;)public class User &#123; private Integer id; private String name; private Integer age;@ImportSource：导入Spring的配置文件，让配置文件里面的内容生效在Spring Boot中，对于我们自己编写的配置文件，是不能自动识别的；想让Spring的配置文件生效，加载进去；需要将@ImportSource加载配置类上。12@ImportSource(locations = &#123;\"classpath:beans.xml\"&#125;)导入Spring的配置文件让其生效SpringBoot推荐给容器中添加组件的方式：推荐全注解1、配置类，使用@Configuration设置为配置类2、使用@Bean给容器添加组件12345678910@Configurationpublic class MyConfig &#123; //将方法中的返回值添加到容器中，容器中这种组件的id为方法名 @Bean public HelloServiceImpl helloService()&#123; System.out.println(\"配置类@Bean给容器中添加组件\"); return new HelloServiceImpl(); &#125;&#125;3、","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"}],"author":"wonghy"},{"title":"Spring Boot之配置文件","slug":"2018-7-2-SpringBoot之配置文件","date":"2019-04-15T03:46:00.279Z","updated":"2019-04-15T03:49:23.264Z","comments":true,"path":"2019/04/15/2018-7-2-SpringBoot之配置文件/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-7-2-SpringBoot之配置文件/","excerpt":"","text":"1、YAML语法：2、配置文件值注入3、配置文件占位符1、随机数12$&#123;random.value&#125;、$&#123;random.int&#125;、$&#123;random.long&#125;$&#123;random.int(10)&#125;、$random.int[1024,65536]&#125;2、占位符获取前面配置的值，如果没有可以用 :（冒号） 指定默认值12person.lastName=hhyperson.dog.name=$&#123;person.hello:hello&#125;_dog4、Profile多环境配置1、多Profile文件application-{profile}.properties/yml默认使用application.properties文件2、yml支持多文档块方式1234567891011121314151617server: port: 8081spring: profiles: active: prod---spring: profiles: devserver: port: 8083---spring: profiles: prodserver: port: 80843、激活使用profile​ 1、在配置文件中指定 spring.profiles.active=dev​ 2、命令行：–spring.profiles.active=dev​ 在run/debug Confgurations中Program arguments中填写–spring.profiles.active=dev​ 在cmd中，java -jar xxxx.jar –spring.profiles.active=dev​ 3、虚拟机参数：​ 在run/debug Confgurations中VM options中填写:​ -Dspring.profiles.active=dev5、配置文件加载位置Spring boot启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件file: ./config/ 根路径的文件夹file: ./classpath: ./config/ 类路径的config文件夹classpath: /优先级由高到低，高优先级的配置会覆盖低优先级的配置Spring boot 会从四个位置全部加载主配置文件：互补配置在运维的时候比较方便：可以通过spring.config.location来改变配置项目已经打包好了，我可以使用命令行参数的形式（–spring.config.location=xxxx），启动项目的时候指定配置文件的新位置；指定配置文件和默认加载的配置文件共同起作用形成互补配置6、外部配置加载顺序优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置1.命令行参数所有的配置都可以在命令行上进行指定java -jar xxx.jar –server.port=8087 –server.context-path=/abc多个配置用空格分开 –配置项=值2.来自java:comp/env的JNDI属性3.Java系统属性（System.getProperties()）4.操作系统环境变量5.RandomValuePropertySource配置的random.*属性值由jar包外向jar包内进行寻找；==优先加载带profile==6.jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件7.jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件==再来加载不带profile==8.jar包外部的application.properties或application.yml(不带spring.profile)配置文件9.jar包内部的application.properties或application.yml(不带spring.profile)配置文件10.@Configuration注解类上的@PropertySource11.通过SpringApplication.setDefaultProperties指定的默认属性所有支持的配置加载来源；参考官方文档7、自动配置原理配置文件能配置的属性参照Spring Boot 启动的时候加载主配置类，开启自动配置功能==@EnableAutoConfiguration====@EnableAutoConfiguration==的作用：精髓：xxxAutoConfiguration：自动配置类给容器添加组件xxxProperties：封装配置文件中相关属性（如果我们对自动配置类中哪些属性 不满意，可以通过在配置文件中配置）","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"}],"author":"wonghy"},{"title":"Spring Boot之基础准备","slug":"2018-06-14-SpringBoot基础准备","date":"2019-04-15T03:46:00.266Z","updated":"2019-04-15T03:49:23.239Z","comments":true,"path":"2019/04/15/2018-06-14-SpringBoot基础准备/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-06-14-SpringBoot基础准备/","excerpt":"","text":"SpringBoot是什么？简化Spring应用开发的一个框架；整个Spring技术栈的大集合；J2EEE开发的一站式解决方案SpringBoot优缺点？快速创建独立运行的Spring项目和主流框架的继承使用嵌入式的Servlet容器，应用无需打包成war包，直接打包成jar包，使用java -jar命令执行starters自动依赖与版本控制大量自动配置，简化开发，也可修改默认值无需配置xml生产环境的运行时应用监控与云计算的天然合成SpringApplication的执行流程流程图：具体步骤：SpringApplication初始化SpringApplicationRunListener","categories":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/categories/Spring-Boot/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"http://yoursite.com/tags/Spring-Boot/"}],"author":"wonghy"},{"title":"缓存机制","slug":"2018-06-07-缓存","date":"2019-04-15T03:46:00.256Z","updated":"2019-04-15T03:49:23.069Z","comments":true,"path":"2019/04/15/2018-06-07-缓存/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-06-07-缓存/","excerpt":"","text":"缓存是什么？MyBatis缓存问题：useCache和flushCache这两个选项有点混淆：flushCache：如果设成true，当语句调用时一级二级缓存都会被清理掉。select语句默认是falseuseCache：如果设成true，语句调用的结果会缓存在二级缓存里。select语句默认是true第三方缓存：","categories":[{"name":"缓存","slug":"缓存","permalink":"http://yoursite.com/categories/缓存/"}],"tags":[{"name":"缓存","slug":"缓存","permalink":"http://yoursite.com/tags/缓存/"}],"author":"wonghy"},{"title":"Git远程仓库","slug":"2018-06-06-git远程仓库","date":"2019-04-15T03:46:00.248Z","updated":"2019-04-15T03:49:23.217Z","comments":true,"path":"2019/04/15/2018-06-06-git远程仓库/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-06-06-git远程仓库/","excerpt":"","text":"用命令行形式来创建一个仓库123456789101112echo \"# MyDemo\" &gt;&gt; README.mdgit initgit add README.mdgit commit -m \"first commit\"git remote add origin https://github.com/Wonghy449/MyDemo.git#由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。git push -u origin master从已有的远程仓库克隆一份123git clone git@github.com:Wonghy449/MyDemo.gitgit push -u origin master","categories":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}],"author":"wonghy"},{"title":"Git命令","slug":"2018-06-06-git命令列表","date":"2019-04-15T03:46:00.241Z","updated":"2019-04-15T03:49:23.116Z","comments":true,"path":"2019/04/15/2018-06-06-git命令列表/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-06-06-git命令列表/","excerpt":"","text":"Git的操作参考：图解Git命令日常使用6个命令新建代码库12345678# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url]配置Git的设置文件为.gitconfig，它可以在用户主目录下，也可以在项目目录下。123456789101112131415161718# 显示当前的Git配置$ git config --list#查看系统configgit config --system --list #查看当前用户（global）配置git config --global --list #查看当前仓库配置信息git config --local --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name &quot;[name]&quot;$ git config [--global] user.email &quot;[email address]&quot;增加/删除文件1234567891011121314151617# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed]代码提交123456789101112131415161718# 提交暂存区到仓库区$ git commit -m [message]# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a# 提交时显示所有diff信息$ git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化$ git commit --amend &lt;file1&gt; &lt;file2&gt; ...分支123456789101112131415161718192021222324252627282930313233343536373839404142434445# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支#注意到上面的Fast-forward信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。$ git merge [branch]$ git merge --no-ff -m \"merge with no-ff\" [branch] # 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]#强制删除没有合并的分支$ git branch -D [name]# 删除远程分支$ git push origin --delete &lt;branch-name&gt;$ git branch -dr &lt;remote/branch&gt;储存分支123456789#存储当前分支工作现场$ git stash#查看存储的工作现场列表$ git stash list#恢复工作现场$ git stash apply stash@&#123;Number&#125;$ git stash pop标签1234567891011121314151617181920# 列出所有tag$ git tag# 新建一个tag在当前commit$ git tag [tag]# 新建一个tag在指定commit$ git tag [tag] [commit]# 查看tag信息$ git show [tag]# 提交指定tag$ git push [remote] [tag]# 提交所有tag$ git push [remote] --tags# 新建一个分支，指向某个tag$ git checkout -b [branch] [tag]查看信息123456789101112131415161718192021222324252627282930313233343536373839404142# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [&lt;file&gt;]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all撤销123456789101112131415161718192021222324252627# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到工作区$ git checkout [commit] [file]# 恢复上一个commit的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]其他12# 生成一个可供发布的压缩包# git archive","categories":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}],"author":"wonghy"},{"title":"Git搭建仓库","slug":"2018-06-06-git搭建仓库","date":"2019-04-15T03:46:00.234Z","updated":"2019-04-15T03:49:23.166Z","comments":true,"path":"2019/04/15/2018-06-06-git搭建仓库/","link":"","permalink":"http://yoursite.com/2019/04/15/2018-06-06-git搭建仓库/","excerpt":"","text":"Git介绍Git 是一个开源的分布式版本控制软件,用以有效、高速的处理从很小到非常大的项目版本管理。 Git 最初是由Linus Torvalds设计开发的，用于管理Linux内核开发。Git 是根据GNU通用公共许可证版本2的条款分发的自由/免费软件，安装参见：http://git-scm.com/参考：深入浅出Git教程Git命令教程高质量的 Git 中文教程，来自国外社区的优秀文章和个人实践Git参考手册windows系统配置git前面安装过程不在阐述，直说下面如何配置git因为Git是分布式版本控制系统，所以需要填写用户名和邮箱作为一个标识。12$ git config --global user.name &quot;xxx&quot;$ git config --global user.email &quot;xxx@xxx&quot;1$ ssh-keygen #按几次enter回车键，出现目录链接，然后找到xx.pub文件，打开文件，复制里面的内容到github中操作git获得git仓库通过命令 git init 把这个目录变成git可以管理的仓库12345# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]这时候你当前目录下会多了一个.git的目录，这个目录是Git来跟踪管理版本的，没事千万不要手动乱改这个目录里面的文件，否则，会把git仓库给破坏了。克隆远程目录，由于是将远程服务器上的仓库完全镜像一份至本地，而不是取某一个特定版本，所以用clone而不是checkout12# 克隆一个项目和它的整个代码历史(版本信息)$ git clone [url]把文件添加到版本库中git版本控制系统可以告诉你每次的改动，只能跟踪文本文件的改动，比如txt文件，网页，所有程序的代码等，只能把二进制文件每次改动串起来，也就是知道图片从1kb变成2kb，但是到底改了啥，版本控制也不知道。我在版本库目录中新建文本文件，如readme.txt12345第一步:使用命令 git add添加到暂存区里面去$ git add readme.txt第二步：用命令 git commit告诉Git，把文件提交到仓库。$ git commit -m &quot;readme.txt提交&quot;查看文件的状态实际使用中你不止只有一个文件，或新增或修改多个文件，可能时间一久就忘了有多少文件需要提交，可以使用 git status 命令来查看当前状态,是否有未提交的文件1$ git status比较当前文件跟版本文件内容git diff用于显示WorkSpace中的文件和暂存区文件的差异12345678910$ git diff readme.txt# 查看已缓存的改动$ git diff --cached# 查看已缓存的与未缓存的所有改动$ git diff HEAD#显示摘要而非整个 diff$ git diff --stat查看历史提交记录1234567891011# 历史纪录是根据时间倒叙排列的$ git log# 一个压缩后的每一条提交记录只占一行的输出$ git log --pretty=oneline# 只看某一个人的提交记录 $ git log --author=bob# ASCII 艺术的树形结构来展示所有的分支, 每个分支都标示了他的名字和标签$ git log --graph --oneline --decorate --all文件4种状态Untracked: 未跟踪, 此文件在文件夹中, 但并没有加入到git库, 不参与版本控制. 通过git add 状态变为Staged.Unmodify: 文件已经入库, 未修改, 即版本库中的文件快照内容与文件夹中完全一致. 这种类型的文件有两种去处, 如果它被修改, 而变为Modified. 如果使用git rm移出版本库, 则成为Untracked文件Modified: 文件已修改, 仅仅是修改, 并没有进行其他的操作. 这个文件也有两个去处, 通过git add可进入暂存staged状态, 使用git checkout 则丢弃修改过, 返回到unmodify状态, 这个git checkout即从库中取出文件, 覆盖当前修改Staged: 暂存状态. 执行git commit则将修改同步到库中, 这时库中的文件和本地文件又变为一致, 文件为Unmodify状态. 执行git reset HEAD filename取消暂存, 文件状态为Modified理解工作区、暂存区和版本库的区别工作区域：Git本地有三个工作区域：工作目录（Working Directory）、暂存区(Stage/Index)、资源库(Repository或Git Directory)。如果在加上远程的git仓库(Remote Directory)就可以分为四个工作区域。文件在这四个区域之间的转换关系如下：Workspace：工作区，就是你平时存放项目代码的地方Index / Stage：暂存区，用于临时存放你的改动，事实上它只是一个文件，保存即将提交到文件列表信息Repository：仓库区（或本地仓库），就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本Remote：远程仓库，托管代码的服务器，可以简单的认为是你项目组中的一台电脑用于远程数据交换工作区：就是你在电脑里能看到的目录。暂存区：英文叫stage, 或index。一般存放在 “.git目录下” 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。","categories":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}],"author":"wonghy"},{"title":"Hello World","slug":"hello-world","date":"2019-04-02T04:21:02.820Z","updated":"2019-04-02T04:21:02.820Z","comments":true,"path":"2019/04/02/hello-world/","link":"","permalink":"http://yoursite.com/2019/04/02/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new \"My New Post\"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment","categories":[],"tags":[]},{"title":"CSS 各种Hack手段","slug":"css-hack","date":"2017-06-24T19:25:24.000Z","updated":"2018-10-20T07:46:51.000Z","comments":false,"path":"2017/06/25/css-hack/","link":"","permalink":"http://yoursite.com/2017/06/25/css-hack/","excerpt":"","text":"随着浏览器的发展，css hack 技术的使用应该越来越少了，但是在某些关键时刻以及综合的WEB应用或者老项目中，可能还需要使用 css hack 技术来解决一些问题。css hack 分类css hack 分类大致有 3 种表现形式：IE条件注释法、CSS属性前缀法以及选择器前缀法。IE 条件注释法（即 HTML 条件注释 Hack）：针对所有IE(注：IE10+ 已经不再支持条件注释)：&lt;!--[if IE]&gt;IE浏览器显示的内容 &lt;![endif]--&gt;；针对 IE6 及以下版本：&lt;!--[if lt IE 6]&gt;只在IE6-显示的内容 &lt;![endif]--&gt;。这类 Hack 不仅对 CSS 生效，对写在判断语句里面的所有代码都会生效。属性前缀法（即类内部 Hack）：例如 IE6 能识别下划线 _ 和星号 *，IE7 能识别星号 *，但不能识别下划线 _，IE6~IE10 都认识 \\9，但 firefox 前述三个都不能认识。选择器前缀法（即选择器 Hack）：例如 IE6 能识别 *html .class{}，IE7 能识别 *+html .class{} 或者 *:first-child+html .class{}。css hack 书写顺序，一般是将适用范围广、被识别能力强的 CSS 定义在前面。条件注释法语法：123&lt;!-- [if &lt;keywords&gt;? IE &lt;version&gt; ?]&gt;HTML 代码块&lt;![endif]--&gt;取值：&lt;keywords&gt;if 条件共包含 6 种选择方式：是否、大于、大于或等于、小于、小于或等于、非指定版本是否：指定是否 IE 或 IE 某个版本。关键字：空大于：选择大鱼指定版本的 IE 版本。关键字：gt大于或等于：选择大于或等于指定版本的 IE 版本。关键字：gte小于：选择小于指定版本的IE版本。关键字：lt小于或等于：选择小于或等于指定版本的IE版本。关键字：lte非指定版本：选择除指定版本外的所有IE版本。关键字：!说明：用于选择 IE 浏览器及IE的不同版本示例：123456789101112131415161718192021222324只在IE下生效&lt;!--[if IE]&gt;这段文字只在IE浏览器显示&lt;![endif]--&gt;只在IE6下生效&lt;!--[if IE 6]&gt;这段文字只在IE6浏览器显示&lt;![endif]--&gt;只在IE6以上版本生效&lt;!--[if gte IE 6]&gt;这段文字只在IE6以上(包括)版本IE浏览器显示&lt;![endif]--&gt;只在IE8上不生效&lt;!--[if ! IE 8]&gt;这段文字在非IE8浏览器显示&lt;![endif]--&gt;非IE浏览器生效&lt;!--[if !IE]&gt;这段文字只在非IE浏览器显示&lt;![endif]--&gt;需要说明的是，IE10和11已经不支持这种条件注释法了。运行上面示例CSS 属性前缀法语法：selector {?property:value?;}取值：_：选择 IE6 及以下。连接线（中划线）（-）亦可使用，为了避免与某些带中划线的属性混淆，所以使用下划线（_）更为合适。*：选择 IE7 及以下。诸如：（+）与（#）之类的均可使用，不过业界对（*）的认知度更高。\\9：选择 IE6+。\\0：选择 IE8+ 和 Opera。[;property:value;];：选择 webkit 核心浏览器（Chrome,Safari）。IE7 及以下也能识别。中括号内外的 3 个分号必须保留，第一个分号前可以是任意规则或任意多个规则。[;color:#f00;]; 与 [color:#f00;color:#f00;]; 与 [margin:0;padding:0;color:#f00;]; 是等价的。生效的始终是中括号内的最后一条规则，所以通常选用第一种写法最为简洁。说明：选择不同的浏览器及版本尽可能减少对 CSS Hack 的使用。Hack 有风险，谨慎使用。一些 CSS Hack 由于浏览器存在交叉认识，所以需要通过层层覆盖的方式来实现对不同浏览器进行 Hack 的。如下面这个例子：12345.test&#123; color:#090\\9; /* For IE8+ */ *color:#f00; /* For IE7 and earlier */ _color:#ff0; /* For IE6 and earlier */&#125;上述 Hack 均需运行在标准模式下，若在怪异模式下运行，这些 Hack 将会被不同版本的 IE 相互识别，导致失效。选择器前缀法语法：&lt;hack&gt;selector{sRules}说明：1234* html .test&#123;color:#090;&#125; /* For IE6 and earlier */* + html .test&#123;color:#ff0;&#125; /* For IE7 */.test:lang(zh-cn)&#123;color:#f00;&#125; /* For IE8+ and not IE */.test:nth-child(1)&#123;color:#0ff;&#125; /* For IE9+ and not IE */上述代码中的3,4两行就是典型的利用能力来进行选择的 CSS Hack。","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"}],"tags":[{"name":"CSS","slug":"CSS","permalink":"http://yoursite.com/tags/CSS/"}]},{"title":"gulp 详解与使用","slug":"gulp","date":"2017-04-30T17:25:24.000Z","updated":"2018-10-20T07:46:51.000Z","comments":false,"path":"2017/05/01/gulp/","link":"","permalink":"http://yoursite.com/2017/05/01/gulp/","excerpt":"什么是 gulpgulp 是一个前端构建工具，它能通过自动执行常见任务，比如编译预处理 CSS ，压缩 JavaScript 和刷新浏览器，来改进网站开发的过程，从而使开发更加快速高效。为什么要用 gulp与 grunt 相比，gulp 无需写一大堆繁杂的配置参数，API（中文 API） 也非常简单，学习起来很容易，而且 gulp 使用的是 nodejs 中 stream 来读取和操作数据，其速度更快。gulp 有庞大的生态圈，且每天都在发展。依靠成千上万可供选择的插件，你可以利用 gulp 自动完成几乎任何事。如何使用 gulpInstalling Gulp新版的 gulp 命令行工具已经改名为 gulp-cli 。如果你之前安装了全局的 gulp 。在使用新的 gulp-cli 之前，执行命令npm rm --global gulp ，将之前的全局 gulp 卸掉。","text":"什么是 gulpgulp 是一个前端构建工具，它能通过自动执行常见任务，比如编译预处理 CSS ，压缩 JavaScript 和刷新浏览器，来改进网站开发的过程，从而使开发更加快速高效。为什么要用 gulp与 grunt 相比，gulp 无需写一大堆繁杂的配置参数，API（中文 API） 也非常简单，学习起来很容易，而且 gulp 使用的是 nodejs 中 stream 来读取和操作数据，其速度更快。gulp 有庞大的生态圈，且每天都在发展。依靠成千上万可供选择的插件，你可以利用 gulp 自动完成几乎任何事。如何使用 gulpInstalling Gulp新版的 gulp 命令行工具已经改名为 gulp-cli 。如果你之前安装了全局的 gulp 。在使用新的 gulp-cli 之前，执行命令npm rm --global gulp ，将之前的全局 gulp 卸掉。Install the gulp command1npm install --global gulp-cliInstall gulp in your devDependenciesRun this command in your project directory1npm install --save-dev gulpCreate a gulpfileCreate a file called gulpfile.js in your project root with these contents:12345var gulp = require('gulp');gulp.task('default', function() &#123; // place code for your default tash here&#125;);Test it outRun the gulp command in your projct directory:1gulpgulp APIgulp 的核心 API 有四个：gulp.task() 、 gulp.src() 、 gulp.dest() 、 gulp.watch() 。gulp APIgulp API 中文下面详细介绍一下：gulp.src()gulp.src() 可以读取你需要操作的文件，相比于 Grunt 主要以文件为媒介来运行它的工作流，gulp 使用的是 Nodejs 中的 stream 流，首先获取到需要的 stream ，然后可以通过 stream 的 pipe() 方法把流导入到你想要的地方，比如 gulp 的插件中，经过插件处理后的流又可以继续导入到其他插件中，当然也可以把流写入到文件中。所以 gulp 是以 stream 为媒介的，它不需要频繁的生成临时文件，这也是 gulp 的速度比 Grunt 快的一个原因。再回到正题上来，gulp.src() 方法正是用来获取流的，但要注意这个流里的内容不是原始的文件流，而是一个虚拟文件对象流（Vinyl files），这个虚拟文件对象中存储着原始文件的路径、文件名、内容等信息，这个我们暂时不用去深入理解，你只需简单的理解可以用这个方法来读取你需要操作的文件就行了。其语法为：1gulp.src(globs [, options])gulp 用到的 glob 的匹配规则以及一些文件匹配技巧。gulp 内部使用了 node-glob 模块来实现其文件匹配功能。我们可以使用下面这些特殊的字符来匹配我们想要的文件：* 匹配文件路径中的 0 个或多个字符，但不会匹配路径分配符，除非路径分隔符出现在末尾** 匹配路径中的 0 个或多个目录及其子目录，需要单独出现，即它左右不能有其他东西了。如果出现在末尾，也能匹配文件。? 匹配文件路径中的一个字符（不会匹配路径分隔符）[…] 匹配方括号中出现的字符中的任意一个，当方括号中第一个字符为 ^ 或 ! 时，则表示不匹配方括号中出现的其他字符中的任意一个，类似 js 正则表达式中的用法。!(pattern|pattern|pattern) 匹配任何与括号中给定的任一模式都不匹配的?(pattern|pattern|pattern) 匹配括号中给定的任一模式 0 次或 1 次，类似于 js 正则中的(pattern|pattern|pattern)?+(pattern|pattern|pattern) 匹配括号中给定的任一模式至少 1 次，类似于正则中的(pattern|pattern|pattern)+*(pattern|pattern|pattern) 匹配括号中的给定的任一模式 0 次或多次，类似于 js 正则中的 (pattern|pattern|pattern)*@(pattern|pattern|pattern) 匹配括号中给定的任一模式 1 次，类似于 js 正则中的(pattern|pattern|pattern)下面以一系列例子来加深理解* 能匹配 a.js 、 x.y 、 abc 、 abc/ ，但不能匹配 a/b.js*.* 能匹配 a.js 、 style.css 、 a.b 、 x.y*/*/*.js 能匹配 a/b/c.js 、 x/y/z.js ，不能匹配 a/b.js 、a/b/c/d.js** 能匹配 abc 、 a/b.js 、 a/b/c.js 、 x/y/z 、x/y/z/a.b ，能用来匹配所有的目录和文件*/.js 能匹配 foo.js 、 a/foo.js 、 a/b/foo.js 、 a/b/c/foo.jsa/**/z 能匹配 a/z 、 a/b/z 、 a/b/c/z 、 a/d/g/h/r/za/**b/z 能匹配 a/b/z 、 a/fb/z ，但不能匹配 a/x/gb/z ，因为只有单 ** 单独出现才能匹配多级目录?.js 能匹配 a.js 、 b.js 、 c.jsa?? 能匹配 a.b 、 abc ，但不能匹配 ab/ ，因为它不会匹配路径分隔符[xyz].js 只能匹配 x.js 、 y.js 、 z.js ，不会匹配 xy.js 、 xyz.js 等，整个中括号只代表一个字符[^xyz].js 能匹配 a.js 、 b.js 、 c.js 等，不能匹配 x.js 、 y.js 、 z.js当有多种匹配模式时可以使用数组12// 使用数组的方式来匹配多种文件gulp.src(['js/*.js','css/*.css','*.html'])使用数组的方式还有一个好处就是可以很方便的使用排除模式，在数组中的单个匹配模式前加上 ! 即是排除模式，它会在匹配的结果中排除这个匹配，要注意一点的是不能在数组中的第一个元素中使用排除模式12gulp.src([*.js,'!b*.js']) //匹配所有 js 文件，但排除掉以 b 开头的 js 文件gulp.src(['!b*.js',*.js]) //不会排除任何文件，因为排除模式不能出现在数组的第一个元素中此外，还可以使用展开模式。展开模式以花括号作为定界符，根据它里面的内容，会展开为多个模式，最后匹配的结果为所有展开的模式想加起来得到的结果。展开的例子如下：a{b,c}d 会展开为 abd 、 acda{b,}c 会展开为 abc 、 aca{0..3}d 会展开为 a0d 、 a1d 、 a2d 、 a3da{b,c{d,e}f}g 会展开为 abg 、 acdfg 、 acefga{b,c}d{e,f}g 会展开为 abdeg 、 acdeg 、 abdfg 、 abdeggulp.dest()gulp.dest() 方法是用来写文件的，其语法为：1gulp.dest(path[, options])path 为写入文件的路径我们给 gulp.dest() 传入的路径参数，只能用来指定要生成的文件的目录，而不能指定生成文件的文件名，它生成文件的文件名使用的是导入到它的文件流自身的文件名，所以生成的文件名是由导入到它的文件流决定的，即使我们给它传入一个带有文件名的路径参数，然后它也会把这个文件名当作是目录名，例如：123var gulp = require(\"gulp\");gulp.src(\"script/jquery.js\").pipe(gulp.dest(\"dist/foo.js\"));// 最终生成的文件路径为 dist/foo.js/jquery.js ，而不是 dist/foo.js要想改变文件名，可以使用插件 gulp-rename下面说说生成的文件路径与我们给 gulp.dest() 方法传入的路径参数之间的关系。gulp.dest(path) 生成的文件路径是我们传入的 path 参数后面再加上 gulp.src() 中有通配符开始出现的那部分路径。例如：12345var gulp = require(\"gulp\");//有通配符开始出现的那部分路径为 **/*.jsgulp.src(\"script/**/*.js\").pipe(gulp.dest(\"dist\"));//最后生成的文件路径为 dist/**/*.js//如果 **/*.js 匹配到的文件为 jquery/jquery.js ，则生成的文件路径为 dist/jquery/jquery.js再举更多一点的例子123456789101112gulp.src(\"script/avalon/avalon.js\").pipe(gulp.dest(\"dist\"));//没有通配符出现的情况，最后生成的文件路径为 dist/avalon.jsgulp.src(\"script/**/underscore.js\").pipe(gulp.dest(\"dist\"));//有通配符开始出现的那部分路径为 **/underscore.js//假设匹配到的文件为script/util/underscore.js//则最后生成的文件路径为dist/util/underscore.jsgulp.src(\"script/*\").pipe(gulp.dest(\"dist\"));//有通配符出现的那部分路径为*//假设匹配到的文件为script/zepto.js//则最后生成的文件路径为dist/zepto.js通过指定 gulp.src() 方法配置参数中的 base 属性，我们可以灵活的来改变 gulp.dest() 生成的文件路径。当我们没有在 gulp.src() 方法配置参数中的 base 属性，base 的默认值为通配符开始出现之前那部分路径，例如：1gulp.src(\"app/src/**/*.css\") //此时base的值为 app/src上面我们说的 gulp.dest() 所生成的文件路径的规则，其实也可以理解成，用我们给 gulp.dest() 传入的路径替换掉 gulp.src() 中的 base 路径，最终得到生成文件的路径。1234gulp.src(\"app/src/**/*.css\").pipe(gulp.dest(\"dist\"));//此时base的值为app/src，也就是说它的base路径为app/src//设该模式匹配到了文件app/src/css/normal.css//用dist替换掉base路径，最终得到dist/css/normal.css所以改变 base 路径后，gulp.dest() 生成的文件路径也会改变123456789gulp.src(\"script/lib/*.js\").pipe(gulp.dest(\"build\"));//没有配置base参数，此时默认的base路径为script/lib//假设匹配到的文件为script/lib/jquery.js//生成的文件路径为build/jquery.jsgulp.src(\"script/lib/*.js\", &#123;base: \"script\"&#125;).pipe(gulp.dest(\"build\"));//配置了base参数，此时base路径为script//假设匹配到的文件为script/lib/jquery.js//此时生成的文件路径为build/lib/jquery.js用 gulp.dest() 把文件流写入文件后，文件流仍然可以继续使用。gulp.task()gulp.task 方法用来定义任务，内部使用的是 Orchestrator ，其语法为：1gulp.task(name[, deps], fn)name 为任务名，如果你需要在命令行中运行你的某些任务，那么，请不要在名字中使用空格。deps 是当前定义的任务需要依赖的其他任务，为一个数组。当前定义的任务会在所有依赖的任务执行完毕后才开始执行。如果没有依赖，则可省略这个参数。fn 为任务函数，我们把任务要执行的代码都要写在里面。该参数也是可选的。1234gulp.task(\"mytask\", [\"array\", \"of\", \"task\", \"names\"], function() &#123; //定义一个有依赖的任务 // Do something&#125;);关于 gulp.task() ，我们需要知道执行多个任务时怎么来控制任务执行的顺序。gulp 中执行多个任务，可以通过任务依赖来实现。例如我想要执行 one ，two ，three 这三个任务，那我们就可以定义一个空的任务，然后把那三个任务当做这个空的任务的依赖就行了：12//只要执行default任务，就相当于把one,two,three这三个任务执行了gulp.task('default',['one','two','three']);如果任务相互之间没有依赖，任务会按你书写的顺序来执行，如果有依赖的话则会先执行依赖的任务。但是如果某个任务所依赖的任务是异步的，就要注意了，gulp 并不会等待那个所依赖的异步任务完成，而是会接着执行后续的任务。例如：1234567891011gulp.task('one', function()&#123; // one是一个异步执行的任务 setTimeout(function()&#123; console.log(\"one is done\"); &#125;,3000);&#125;);//two任务虽然依赖于one任务，但并不会等到one任务中的异步操作完成后再执行gulp.task(\"two\", [\"one\"], function()&#123; console.log(\"two is done\");&#125;);上面的例子中我们执行 two 任务时，会先执行 one 任务，但不会去等待 one 任务中的异步操作完成后再执行 two 任务，而是紧接着执行 two 任务。因为 one 任务耗时 3 秒，所以 two 任务会在 one 任务中的异步操作完成之前就执行了。那如果我们想等待异步任务中的异步操作完成后再执行后续的任务，该怎么做呢？有三种方法可以实现：第一：在异步操作完成后执行一个回调函数来通知 gulp 这个异步任务已经完成，这个回调函数就是任务函数的第一个参数。12345678910111213gulp.task(\"one\", function(cb) &#123; //cb为任务函数提供的回调，用来通知任务已经完成 //one是一个异步执行的任务 setTimeout(function()&#123; console.log(\"one is done\"); cb(); //执行回调，表示这个异步任务已经完成 &#125;,5000);&#125;);//这时two任务会在one任务中的异步操作完成后再执行gulp.task(\"two\", [\"one\"], function()&#123; console.log(\"two is done\");&#125;);第二：定义任务时返回一个流对象。适用于任务就是操作 gulp.src 获取到的流的情况。1234567891011gulp.task(\"one\", function(cb) &#123; var stream = gulp.src(\"client/**/*.js\") .pipe(dosomething()) //dosomething()中有某些异步操作 .pipe(gulp.dest(\"build\")); return stream;&#125;);// 这是two任务会在one任务中的异步操作完成后再执行gulp.task(\"two\", [\"one\"], function()&#123; console.log(\"two is done\");&#125;);第三：返回一个 promise 对象，例如：12345678910111213var Q = require('q'); //一个著名的异步处理的库 https://github.com/kriskowal/qgulp.task(\"one\", function(cb)&#123; var deferred = Q.defer(); //做一些异步操作 setTimeout(function()&#123; deferred.resolve(); &#125;,5000); return deferred.promise;&#125;);gulp.task(\"two\", [\"one\"], function() &#123; console.log(\"two is done\");&#125;);关于 gulp.task() ，主要的就是要清除当依赖异步任务时要如何处理。gulp.watch()gulp.watch() 用来监视文件的变化，当文件发生变化后，我们可以利用它来执行相应的任务，例如文件压缩等。其语法为：1gulp.watch(glob[, opts], tasks);glob 为要监视的文件匹配模式，规则和用法与 gulp.src() 方法中的 glob 相同。opts 为一个可选的配置对象，通常不需要用到。tasks 为文件变化后要执行的任务，为一个数组1234567gulp.task(\"uglify\", function() &#123; //do something&#125;);gulp.task(\"reload\", function() &#123; //do something&#125;);gulp.watch(\"js/**/*.js\", [\"uglify\",\"reload\"]);gulp.watch(glob [,opts, cb])glob 和 opts 参数与第一种用法相同cb 参数为一个函数。每当监视的文件发生变化时，就会调用这个函数，并且会给它传入一个对象，该对象包含了文件变化的一些信息，type 属性为变化的类型，可以是 added 、changed 、deleted ，path 属性为发生变化的文件的路径1234gulp.watch(\"js/**/*.js\", function(event)&#123; console.log(event.type); //变化类型added为新增，deleted为删除，changed为改变 console.log(event.path); //变化的文件的路径&#125;);gulp 的插件gulp 本身虽然不能完成很多任务，但它有大量插件可用，我们可以在 插件页面 或者在 npm 搜索 gulpplugin 。列一些很棒的 plugin ：JSHint ： js代码检查分析工具gulp-coffee ： 编译CoffeeScriptgulp-mocha ： 执行Mocha测试gulp-bump ： 更新版本号gulp-sass ： sass 编译browser-sync ： 浏览器自动刷新gulp-uglify ： 代码压缩gulp-concat ： 合并gulp-eslint ： 支持 ES6 JSXgulp 命令行参数-v 或 --version 会显示全局和项目本地所安装的 gulp 版本号--require &lt;module path&gt; 将会在执行之前 require 一个模块。这对于一些语言编译器或者需要其他应用的情况来说很有用。你可以使用多个 --require--gulpfile &lt;gulpfile path&gt; 手动指定一个 gulpfile 的路径，这在你有很多个 gulpfile 的时候很有用。这也会将 CWD 设置到该 gulpfile 所在目录--cwd &lt;dir path&gt; 手动指定 CWD 。定义 gulpfile 查找的位置，此外，所有的相应的依赖（require）会从这里开始计算相对路径-T 或 --tasks 会显示所指定 gulpfile 的 task 依赖树--tasks-simple 会以纯文本的方式显示所载入的 gulpfile 中的 task 列表--color 强制 gulp 和 gulp 插件显示颜色，即便没有颜色支持--no-color 强制不显示颜色，即便检测到有颜色支持--silent 禁止所有的 gulp 日志命令行会在 process.env.INIT_CW 中记录它是从哪里被运行的。tip1、gulp 写进项目 package.json 文件的依赖有什么作用方便别人查看你项目中有些什么依赖，而且在项目目录下执行 npm install 命令会安装项目 package.json 中的所有依赖模块，这样就能简化项目的安装程序了，不用一个一个模块去安装啊。2、gulp 中着重了解 gulp.task() 如何处理依赖任务是耗时操作或者异步操作的情况。","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"}],"tags":[{"name":"gulp","slug":"gulp","permalink":"http://yoursite.com/tags/gulp/"}]},{"title":"AMD，CMD 规范详解","slug":"amd-cmd","date":"2017-03-19T19:25:24.000Z","updated":"2018-10-20T07:46:51.000Z","comments":false,"path":"2017/03/20/amd-cmd/","link":"","permalink":"http://yoursite.com/2017/03/20/amd-cmd/","excerpt":"当我们了解了 CommonJS 以后，CommonJS 规范加载模块是同步的，也就是说，只有加载完成，才能执行后面的操作。由于 Node.js 主要用于服务器编程，模块文件一般都已经存在于本地硬盘，所以加载起来比较快，不用考虑非同步加载的方式，所以 CommonJS 规范比较适用。但是，如果是浏览器环境，要从服务器端加载模块，这时就必须采用非同步模式。为什么呢？","text":"当我们了解了 CommonJS 以后，CommonJS 规范加载模块是同步的，也就是说，只有加载完成，才能执行后面的操作。由于 Node.js 主要用于服务器编程，模块文件一般都已经存在于本地硬盘，所以加载起来比较快，不用考虑非同步加载的方式，所以 CommonJS 规范比较适用。但是，如果是浏览器环境，要从服务器端加载模块，这时就必须采用非同步模式。为什么呢？12var math = require('math');math.add(2, 3);第二行 math.add(2, 3)，在第一行 require(&#39;math&#39;) 之后运行，因此必须等 math.js 加载完成。也就是说，如果加载时间很长，整个应用就会停在那里等。对于浏览器，这是一个大问题，因为模块都放在服务器端。等待时间取决于网速的快慢，可能要等很长时间，浏览器处理“假死”状态。因此，浏览器端的模块，不能采用“同步加载”，只能采用“异步加载”。AMDAMD 规范 则是异步加载模块，允许指定回调函数。因此浏览器端一般采用 AMD 规范。AMD(Asynchronous module definition)： 异步模块定义。类似的还有 CommonJS Modules/2.0 规范，是 BravoJS 在推广过程中对模块定义的规范化产出。CMD(Common module definition)：通用模块定义。目前这些规范的实现都能达成浏览器端模块化开发的目的。服务端JS浏览器端JS相同的代码需要多次执行代码需要从一个服务器端分发到多个客户端执行CPU 和内存资源是瓶颈宽带是瓶颈加载时从磁盘中加载加载时需要通过网络加载AMD 的诞生，就是为了解决这两个问题：1.实现 js 文件的异步加载，避免网页失去响应2.管理模块之间的依赖性，便于代码的编写和维护AMD(异步模块定义)主要为前端 JS 的表现指定规范。它采用异步方式加载模块，模块的加载不影响它后面语句的运行。所有依赖这个模块的语句，都定义在一个回调函数中，等到加载完成之后，这个回调函数才会运行。AMD 也采用 require() 语句加载模块，但是不同于 CommonJS，它要求两个参数：1require([module], callback);第一个参数[module]，是一个数组，里面的成员就是要加载的模块；第二个参数 callback，则是加载成功之后的回调函数：123require(['math'], function (math) &#123; math.add(2, 3);&#125;);实现 AMD 规范的加载器其实是挺多的，目前，主要有两个 Javascript 库实现了 AMD 规范：require.js 和 curl.js。不过多数人还是用 require.js 。另外如果对 ES6 的模块感兴趣，可以考虑 my.js ，是按照 ES6 草案的 module/loader 规范实现的。AMD 是 require.js 在推广过程中对模块定义的规范化产出。推荐学习 require.js。AMD 模块的写法require.js 加载的模块，采用 AMD 规范。也就是说，模块必须按照 AMD 的规定来写。具体来说，就是模块必须采用特定的 define() 函数来定义。如果一个模块不依赖其他模块。那么可以直接定义在 define() 函数之中。假定现在有一个 math.js 文件，它定义了一个 math 模块。那么，math.js 就要这样写：123456789// math.jsdefine(function ()&#123; var add = function (x,y)&#123; return x+y; &#125;; return &#123; add: add &#125;;&#125;);加载方法如下：1234// main.jsrequire(['math'], function (math)&#123; alert(math.add(1,1));&#125;);如果这个模块还依赖其他模块，那么 define() 函数的第一个参数，必须是一个数组，指明该模块的依赖性。12345678define(['myLib'], function(myLib)&#123; function foo()&#123; myLib.doSomething(); &#125; return &#123; foo : foo &#125;;&#125;);当 require() 函数加载上面这个模块的时候，就会先加载 myLib.js 文件。加载非规范的模块理论上，require.js 加载的模块，必须是按照 AMD 规范、用 define() 函数定义的模块。但是实际上，虽然已经有一部分流行的函数库（比如 jQuery ）符合 AMD 规范，更多的库并不符合。那么，require.js 是否能够加载非规范的模块呢？回答是可以的。这样的模块在用 require() 加载之前，要先用 require.config() 方法，定义它们的一些特征。举例来说，underscore 和 backbone 这两个库，都没有采用 AMD 规范编写。如果要加载它们的话，必须先定义它们的特征。1234567891011require.config(&#123; shim: &#123; 'underscore': &#123; exports: '_' &#125;, 'backbone': &#123; deps: ['underscore', 'jquery'], exports: 'Backbone' &#125; &#125;&#125;);require.config() 接受一个配置对象，这个对象除了有前面说过的 paths 属性之外，还有一个 shim 属性，专门用来配置不兼容的模块。具体来说，每个模块要定义：（1）exports 值（输出的变量名），表明这个模块外部调用时的名称；（2）deps 数组，表明该模块的依赖性。比如，jQuery 的插件可以这样定义：123456shim: &#123; 'jquery.scroll': &#123; deps: ['jquery'], exports: 'jQuery.fn.scroll' &#125;&#125;CMDCMD 是 sea.js 在推广过程中对模块定义的规范化产出。CMD 模块定义在 CMD 规范中，一个模块就是一个文件。define 是一个全局函数，用来定义模块。define 接受 factory 参数，factory 可以是一个函数，也可以是一个对象或字符串。factory 为对象、字符串时，表示模块的接口就是该对象、字符串。比如可以定义一个 JSON 数据模块：1define(&#123;\"foo\": \"bar\"&#125;);也可以通过字符串定义模板模块：1define('I am a template.My name is &#123;&#123;name&#125;&#125;.');factory 为函数时，表示是模块的构造方法。执行该构造方法，可以得到模块向外提供的接口。factory 是一个函数，有三个参数，function(require, exports, module)1、require 是一个方法，接受模块标识作为唯一参数，用来获取其他模块提供的接口：require(id)2、exports 是一个对象，用来向外提供模块接口3、module 是一个对象，上面存储了与当前模块相关联的一些属性和方法1234567define(function(require, exports, module) &#123; var a = require('./a'); a.doSomething(); // 依赖就近书写，什么时候用到什么时候引入 var b = require('./b'); b.doSomething();&#125;);建议写一写 SeaJS 的 CMD 规范，与 AMD 非常类似，在国内的影响力非常大，但是个人觉得 SeaJS 比 RequireJS 好很多，另外由于是国人开发的，交流也非常方便，可以看到 github 上的更新、互动非常频繁。AMD 与 CMD 的区别区别：对于依赖的模块，AMD 是提前执行，CMD 是延迟执行。不过 RequireJS 从2.0开始，也改成了可以延迟执行（根据写法不同，处理方式不同）。CMD 推崇 as lazy as possible.CMD 推崇依赖就近，AMD 推崇依赖前置","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"}]},{"title":"Spring Cloud（一）：浅谈微服务","slug":"Spring Cloud（一）：浅谈微服务","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-11T03:25:10.770Z","comments":true,"path":"2017/03/07/Spring Cloud（一）：浅谈微服务/","link":"","permalink":"http://yoursite.com/2017/03/07/Spring Cloud（一）：浅谈微服务/","excerpt":"","text":"什么是微服务架构呢？​ 微服务是系统架构上的一种设计风格，它是将一个原本独立的系统拆分成多个小型服务，这些小型服务都在各自独立的进程中运行，服务之间通过基于HTTP的RESTful API进行通信协作。被拆分成的每一个小型服务都是围绕着系统中某一项或者一些耦合度较高业务功能进行构建，并且每个服务都维护自身的数据存储、业务开发、自动化测试以及独立的部署机制。Spring Cloud 简介Spring Cloud 是一个基于Spring Boot 实现的微服务架构开发工具。它为微服务架构中涉及的配置管理、服务治理、断路器、智能路由、微代理、控制总线、全局锁、决策竞选、分布式会话和集群状态管理等操作提供一种简单的开发方式。Spring Cloud的优势微服务的框架那么多比如：dubbo、Kubernetes，为什么就要使用Spring Cloud的呢？产出于spring大家族，spring在企业级开发框架中无人能敌，来头很大，可以保证后续的更新、完善。有Spring Boot 这个独立干将可以省很多事，大大小小的活Spring Boot都搞的挺不错。作为一个微服务治理的大家伙，考虑的很全面，几乎服务治理的方方面面都考虑到了，方便开发开箱即用。Spring Cloud 活跃度很高，教程很丰富，遇到问题很容易找到解决方案轻轻松松几行代码就完成了熔断、均衡负责、服务中心的各种平台功能核心成员Spring Cloud Netflix这可是个大boss，地位仅次于老大，老大各项服务依赖与它，与各种Netflix OSS组件集成，组成微服务的核心，它的小弟主要有Eureka, Hystrix, Zuul, Archaius… 太多了Netflix EurekaNetflix HystrixNetflix ZuulNetflix ArchaiusSpring Cloud ConfigSpring Cloud BusSpring Cloud for Cloud FoundrySpring Cloud ClusterSpring Cloud Consul","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"MySQL优化","slug":"2018-06-30-MySQL优化","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-15T04:10:29.485Z","comments":true,"path":"2017/03/07/2018-06-30-MySQL优化/","link":"","permalink":"http://yoursite.com/2017/03/07/2018-06-30-MySQL优化/","excerpt":"","text":"","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"Spring Cloud（二）：服务治理Eureka","slug":"Spring Cloud（二）：服务治理Eureka","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-11T03:25:10.747Z","comments":true,"path":"2017/03/07/Spring Cloud（二）：服务治理Eureka/","link":"","permalink":"http://yoursite.com/2017/03/07/Spring Cloud（二）：服务治理Eureka/","excerpt":"","text":"什么是Spring Cloud Eureka？Spring Cloud Eureka 是 Spring Cloud Netflix微服务套件中的一部分，是在Eureka开源组件进一步封装。服务中心又称注册中心，管理各种服务功能包括服务的注册、发现、熔断、负载、降级等，比如dubbo admin后台的各种功能。Eureka 是一个基于 REST 的服务，主要在 AWS 云中使用, 定位服务来进行中间层服务器的负载均衡和故障转移。Eureka的基本架构有三种角色。Register Service:服务注册中心，它是一个Eureka Server，提供服务注册和发现的功能失效剔除Eureka Server在启动的时候会创建一个定时任务，默认每隔30秒将当前清单中超时90秒没有续约的服务剔除出去。自我保护服务注册到Eureka Server后，会维护一个心跳连接。Eureka Server在运行期间，会统计心跳失败的比例在15分钟内是否低于85%，如果出现低于的情况，Eureka Server 会将当前的实例注册信息保护起来，让这些实例不会过期。1EMERGENCY! EUREKA MAY BE INCORRECTLY CLAIMING INSTANCES ARE UP WHEN THEY&apos;RE NOT. RENEWALS ARE LESSER THAN THRESHOLD AND HENCE THE INSTANCES ARE NOT BEING EXPIRED JUST TO BE SAFE.Provider Service:服务提供者，它是一个Eureka Client，提供服务服务注册“服务提供者”启动的时候发送REST请求将自己注册到Eureka Server，同时带上自身的元数据信息。Eureka Server接收到REST请求后，将元数据存储到一个双层结构Map中，第一层的key是服务名，第二层key是具体服务的实例名。服务同步由于服务注册中心之间因互相注册为服务，当服务提供者发送注册请求到一个服务注册中心，它会将该请求转发给集群中相连的其他注册中心，从而实现注册中心之间的服务同步。服务续约注册完服务之后，服务提供者会维护一个心跳用来持续告诉Eureka Server，我还活着，防止Eureka Server 剔除任务将服务实例从服务列表中排除出去。Comsumer Service:服务消费者，它是一个Eureka Client，消费服务获取服务启动服务消费者，它发送一个REST请求给服务注册中心，获取注册服务清单。Eureka Server会维护一份只读的服务清单返回给客户端，同时清单会每隔30秒更新一次。服务调用服务消费者在获取服务清单后，通过服务名可以获取具体服务的实例名和实例的元数据信息。（每个服务客户端需要注册到一个Zone中，每个客户端对应一个Region和一个Zone。在服务调用的时候，优先访问同处一个Zone的服务提供方，如果访问不到，就访问其他Zone）服务下线当服务实例进行正常的关闭操作时，它会触发一个服务下线的REST请求给Eureka Server，服务端接收到请求后，将该服务状态设置为下线（DOWN），并把下线的事件传播出去。服务流程是先启动注册中心，服务提供者生产服务并注册到服务中心中，消费者从服务中心中获取服务并执行什么是服务治理？服务治理是微服务架构中最为核心和基础的模块，它主要用来实现各个微服务实例的自动化注册与发现。服务注册在服务治理的框架中，通常会构建一个服务注册中心，每个服务单元向注册中心登记自己提供的服务，将主机与端口好、版本号、通信协议等一些附加信息告知注册中心，注册中心按服务名分类组织服务名单。服务注册中心还需要以==心跳的方式==去检测清单中的服务是否可用，若不可用需要从服务清单中剔除掉。服务发现服务间的调用是通过向服务名发起请求调用实现。调用方向服务注册中心咨询服务，并获得所有服务的实例清单，以实现对具体服务实例的访问。当服务发起调用的时候，就从这份清单用某种轮询策略去除一个位置来进行服务调用。环境搭建搭建服务注册中心123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.SR1&lt;/version&gt; &lt;!--这个springboot2.x以上，springcloud指定的版本--&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt;123456789101112server: port: 8761eureka: instance: hostname: localhost client: #代表不向注册中心注册自己 register-with-eureka: false #注册中心是维护服务实例，不需要去检索服务 fetch-registry: false service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/123@EnableEurekaServer@SpringBootApplicationpublic class EurekaServerApplication &#123;&#125;客户端1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;123456789server: port: 8762spring: application: name: eureka-clienteureka: client: service-url: defaultZone: http://localhost:8761/eureka/123@SpringBootApplication@EnableEurekaClientpublic class EurekaClientApplication&#123;&#125;高可用注册中心（集群）服务端application.yml：123456789101112131415161718192021222324252627---spring: profiles: peer1 application: name: eureka-haserver: port: 8761eureka: instance: hostname: peer1 client: service-url: defaultZone: http://peer2:8762/eureka/---spring: profiles: peer2 application: name: eureka-haserver: port: 8762eureka: instance: hostname: peer2 client: service-url: defaultZone: http://peer1:8761/eureka/电脑配置：因为是在本地搭建Eureka Server 集群，所以需要修改本地的host o Windows 系统的电脑在C: /windows/system32/drivers/etc/hosts 中修改， Mac 系统的电脑通过终端vim/etc/hosts 进行编辑修改12127.0.0.1 peer1127.0.0.1 peer2启动peer1,peer2：12java -jar xx.jar --spring.profiles.active=peer1java -jar xx.jar --spring.profiles.active=peer2","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"Spring Cloud（五）：Spring Cloud Zuul","slug":"Spring Cloud（五）：Spring Cloud Zuul","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-11T03:25:10.816Z","comments":true,"path":"2017/03/07/Spring Cloud（五）：Spring Cloud Zuul/","link":"","permalink":"http://yoursite.com/2017/03/07/Spring Cloud（五）：Spring Cloud Zuul/","excerpt":"","text":"什么是Spring Cloud Zuul？Zuul作为路由网关组件，将所有服务的API接口统一聚合，并且一起对外暴露。外界不用知道内部的各个服务相互调用的复杂性，从而保护了内部微服务单元的API接口，防止被外界直接调用，导致服务的敏感信息暴露。工作原理Zuul是通过Servlet实现的，通过自定义的ZuulServlet（这一点有点像springmvc的dispatchServlet）来对请求进行控制。ZuulServlet 的作用是初始化ZuulFilter，并编排这些ZuulFilter 的执行顺序。Zuul的核心就是一堆的过滤器，可以在Http请求和响应执行一堆过滤器。大概流程应该是这样的：当一个客户端Request 请求进入Zuul 网关服务时，网关先进入“pre filter”，进行一系列的验证、操作或者判断。然后给“routing filter ”进行路由转发，转发到具体的服务实例进行逻辑处理、返回数据。当具体的服务处理完后，最后由“post filter “进行处理， 该类型的处理器处理完之后，将Response 信息返回给客户端。准备工作通过配置文件我们知道，Spring Cloud Zuul 将自己作为一个服务注册到了Eureka。这也就意味着Zuul可以拿到所有注册到Eureka的其他服务的信息。Zuul为这些服务创建了默认的路由规则：/{servicename}/**POM 配置1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt;&lt;/dependency&gt;配置文件1234567891011zuul: routes: hiapi: #自己定义的 path: /hiapi/** #就可以将指定类型的请求Uri 路由到指定的Serviceld serviceid: eureka-client ribbonapi: path: /ribbonapi/** serviceid: eureka-ribbon-client feignapi: path: /feignapi/** serviceid : eureka-feign-client启动类123456789@EnableZuulProxy@EnableEurekaClient@SpringBootApplicationpublic class ApiGatewayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ApiGatewayApplication.class, args); &#125;&#125;","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"Git命令列表","slug":"2018-06-06-git冲突解决","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-15T03:49:23.187Z","comments":true,"path":"2017/03/07/2018-06-06-git冲突解决/","link":"","permalink":"http://yoursite.com/2017/03/07/2018-06-06-git冲突解决/","excerpt":"","text":"现在有master和dev两个分支​ master和dev分支都有对同一个文件进行了修改并提交​ 当master进行==git merge dev== 合并操作时，会出现冲突12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADCreating a new branch is quick &amp; simple.=======Creating a new branch is quick AND simple.&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1三支合并如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。","categories":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"设计模式","slug":"设计模式","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-15T03:45:37.510Z","comments":true,"path":"2017/03/07/设计模式/","link":"","permalink":"http://yoursite.com/2017/03/07/设计模式/","excerpt":"","text":"[TOC]一、设计模式的六大原则总原则：开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。1、单一职责原则不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。2、里氏替换原则（Liskov Substitution Principle）里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科历史替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。3、依赖倒转原则（Dependence Inversion Principle）这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。（抽象不应该依赖细节，细节应该依赖于抽象）4、接口隔离原则（Interface Segregation Principle）这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。5、迪米特法则（最少知道原则）（Demeter Principle）就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。6、合成复用原则（Composite Reuse Principle）原则是尽量首先使用合成/聚合的方式，而不是使用继承。使用“Has-A”和“Is-A”来判断:“Is－A”代表一个类是另外一个类的一种，可以使用继承关系，而“Has-A”代表一个类是另外一个类的一个角色，而不是另外一个类的特殊种类。二、设计模式分类Java中设计模式（java design patterns）通常有23种。模式可以分成3类：创建型、行为型和结构型。创建型模式创建型模式涉及对象的实例化，特点是不让用户代码依赖于对象的创建或者排列方式，避免用户直接使用new创建对象。创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。1、单例模式单例对象能保证在一个JVM中，该对象只有一个实例存在。好处在于：某些类创建比较频繁，对于一些大型的对象，这是一笔很大的系统开销。省去了new操作符，降低了系统内存的使用频率，减轻GC压力。有些类如交易所的核心交易引擎，控制着交易流程，如果该类可以创建多个的话，系统完全乱了。（比如一个军队出现了多个司令员同时指挥，肯定会乱成一团），所以只有使用单例模式，才能保证核心交易服务器独立控制整个流程。饿汉式单例类12345678910111213141516public class EagerSingleton &#123; //饿汉单例模式 //在类加载时就完成了初始化，所以类加载较慢，但获取对象的速度快 private static EagerSingleton instance = new EagerSingleton(); /** * 私有默认构造子 */ private EagerSingleton()&#123;&#125; /** * 静态工厂方法 * 静态，不用同步（类加载时已初始化，不会有多线程的问题） */ public static EagerSingleton getInstance()&#123; return instance; &#125;&#125;饿汉式是典型的空间换时间，当类装载的时候就会创建类的实例，不管你用不用，先创建出来，然后每次调用的时候，就不需要再判断，节省了运行时间。所以饿汉式是线程安全的懒汉式单例类1234567891011121314151617181920public class LazySingleton &#123; //懒汉式单例模式 //比较懒，在类加载时，不创建实例，因此类加载速度快，但运行时获取对象的速度慢 //静态私用成员，没有初始化 private static LazySingleton instance = null; /** * 私有默认构造子 */ private LazySingleton()&#123;&#125; /** * 静态工厂方法 * 同步保证多线程时的正确性（因为类变量不是在加载时初始化的） */ public static synchronized LazySingleton getInstance()&#123; if(instance == null)&#123; instance = new LazySingleton(); &#125; return instance; &#125;&#125;懒汉式是典型的时间换空间,就是每次获取实例都会进行判断，看是否需要创建实例，浪费判断的时间。由于懒汉式的实现是涉及到线程安全的问题，所以一般会使用关键字synchronized来实现线程安全，这样会降低整个访问的速度，而且每次都要判断。2、简单工厂方法模式简单工厂模式实现了生成产品类的代码跟客户端代码分离，在工厂类中你可以添加所需的生成产品的逻辑代码，但是问题来了，优秀的java代码是符合“开放-封闭”原则的，也就是说对扩展开发，对修改关闭，如果你要加一个产品类C，你就要修改工厂类里面的生成产品的代码，在这里你就要增加if-else判断。严重违反了开闭原则12345678910111213141516171819202122232425262728293031323334353637public interface Product &#123; //声明类所需继承的共同接口，也可以是抽象类&#125;public class ProductA implements Product &#123; public ProductA() &#123; System.out.println(\"ProductA\"); &#125;&#125;public class ProductB implements Product&#123;&#125;//工厂类public class Factory &#123; //可以在工厂类中添加任何你所需要的逻辑 public static Product create(String str) &#123; //生成ProductA if(str.equalsIgnoreCase(\"ProductA\")) &#123; return new ProductA(); &#125; else //生成ProductB if(str.equalsIgnoreCase(\"ProductB\")) &#123; return new ProductB(); &#125; return null; &#125;&#125;//客户端public class Client &#123; public static void main(String[] args) &#123; //调用Factory的静态方法生成所要的类 Factory.create(\"productA\"); Factory.create(\"ProductB\"); &#125;&#125;3、抽象工厂方法模式工厂模式 与 抽象工厂模式的区别：工厂方法模式：一个抽象产品类，可以派生出多个具体产品类。一个抽象工厂类，可以派生出多个具体工厂类。每个具体工厂类只能创建一个具体产品类的实例。抽象工厂模式：(多个产品线，你想换什么产品配置，直接建一个工厂就行了)多个抽象产品类，每个抽象产品类可以派生出多个具体产品类。一个抽象工厂类，可以派生出多个具体工厂类。每个具体工厂类可以创建多个具体产品类的实例，也就是创建的是一个产品线下的多个产品类图：抽象工厂有个弊端：产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的工厂里加代码，又要在具体的里面加代码。可以使用反射+配置文件来解决抽象工厂方法中更换数据库替换的方案12345678910111213141516171819202122232425262728293031public class DataAccess &#123; public String getDb()&#123; Properties prop = new Properties(); String db = \"\"; try &#123; InputStream in = DataAccess.class.getClassLoader().getResourceAsStream(\"sql.properties\"); prop.load(in); db = prop.getProperty(\"db\"); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return db; &#125; //反射到对应的Class public Class&lt;?&gt; getSqlFactory(String name) throws ClassNotFoundException &#123; String className = \"com.hhy.algorithm.designPattern.AbstractFactory.\"+getDb()+name; System.out.println(className); Class&lt;?&gt; c1 = Class.forName(className); System.out.println(c1.getName()); return c1; &#125; public IUser CreateUser() throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; Class&lt;?&gt; c1 = getSqlFactory(\"User\"); //获取实例 OracleUser oracleUser = (OracleUser) c1.newInstance(); return oracleUser; &#125;&#125;4、原型模式原理：用原型实例制定创建对象的种类，并且通过拷贝这些原型创建新的对象。该模式不用重新初始化对象，而是动态地获取对象运行时的状态类图：深复制与浅复制：浅复制：被复制对象的所有变量都含有与原来的对象相同的值，而所有的对其他对象的引用都仍然指向原来的对象深复制：把引用对象的变量指向复制过的新对象，而不是原有的被引用的对象。1234567891011121314151617181920212223242526272829303132333435363738public class Prototype implements Cloneable,Serializable &#123; private String name; private User user; /** * 浅复制 * 如果字段是值类型，对该字段执行逐位复制；如果字段是引用类型，则复制引用但不复制引用的对象 * 因此，原始对象跟它副本都是引用同一个对象 * @return * @throws CloneNotSupportedException */ @Override protected Object clone() throws CloneNotSupportedException &#123; Prototype proto = (Prototype) super.clone(); return proto; &#125; /** * 深复制 * 所有需要复制的对象都需要实现java.io.Serializable接口。 * 对象被序列化，然后又被反序列化。反序列化的对象就成了一个深克隆的结果 * @return * @throws IOException * @throws ClassNotFoundException */ public Object deepClone() throws IOException, ClassNotFoundException &#123; /* 写入当前对象的二进制流 */ ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); /* 读出二进制流产生的新对象 */ ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return ois.readObject(); &#125;&#125;原型模式的优缺点：优点：原型模式允许在运行时动态改变具体的实现类型。原型模式可以在运行期间，由客户来注册符合原型接口的实现类型，也可以动态地改变具体的实现类型，看起来接口没有任何变化，但其实运行的已经是另外一个类实例了。因为克隆一个原型就类似于实例化一个类。缺点：原型模式最主要的缺点是每一个类都必须配备一个克隆方法。配备克隆方法需要对类的功能进行通盘考虑，这对于全新的类来说不是很难，而对于已经有的类不一定很容易，特别是当一个类引用不支持序列化的间接对象，或者引用含有循环结构的时候。5、建造者模式（生成器模式）原理：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。所以当创建复杂对象的算法应该独立于该对象的组成部分以及它们的装配方式时使用的模式类图：抽象建造者（Builder）角色：给 出一个抽象接口，以规范产品对象的各个组成成分的建造。一般而言，此接口独立于应用程序的商业逻辑。模式中直接创建产品对象的是具体建造者 (ConcreteBuilder)角色。具体建造者类必须实现这个接口所要求的两种方法：一种是建造方法(buildPart1和 buildPart2)，另一种是返还结构方法(retrieveResult)。一般来说，产品所包含的零件数目与建造方法的数目相符。换言之，有多少零件，就有多少相应的建造方法。具体建造者（ConcreteBuilder）角色：担任这个角色的是与应用程序紧密相关的一些类，它们在应用程序调用下创建产品的实例。这个角色要完成的任务包括：1.实现抽象建造者Builder所声明的接口，给出一步一步地完成创建产品实例的操作。2.在建造过程完成后，提供产品的实例。导演者（Director）角色：担任这个角色的类调用具体建造者角色以创建产品对象。应当指出的是，导演者角色并没有产品类的具体知识，真正拥有产品类的具体知识的是具体建造者角色。产品（Product）角色：产品便是建造中的复杂对象。一般来说，一个系统中会有多于一个的产品类，而且这些产品类并不一定有共同的接口，而完全可以是不相关联的。一般来说，每有一个产品类，就有一个相应的具体建造者类。这些产品应当有一样数目的零件，而每有一个零件就相应地在所有的建造者角色里有一个建造方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//具体建造者public class ConcreteBuilder implements Builder &#123; private Product product = new Product(); /** * 产品零件建造方法1 */ @Override public void buildPart1() &#123; //构建产品的第一个零件 product.setPart1(\"编号：9527\"); &#125; /** * 产品零件建造方法2 */ @Override public void buildPart2() &#123; //构建产品的第二个零件 product.setPart2(\"名称：XXX\"); &#125; /** * 产品返还方法 */ @Override public Product retrieveResult() &#123; return product; &#125;&#125;//导演类public class Director &#123; /** * 持有当前需要使用的建造器对象 */ private Builder builder; /** * 构造方法，传入建造器对象 * @param builder 建造器对象 */ public Director(Builder builder)&#123; this.builder = builder; &#125; /** * 产品构造方法，负责调用各个零件建造方法 */ public void construct()&#123; builder.buildPart1(); builder.buildPart2(); &#125;&#125;行为型模式通过行为型模式，可以更加清晰地划分类与对象的职责，描述类或对象怎样进行交互和职责分配。并研究系统在运行时实例对象之间的交互。在系统运行时，对象并不是孤立的，它们可以通过相互通信与协作完成某些复杂功能，一个对象在运行时也将影响到其他对象的运行。行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。1、策略模式2、模板方法模式3、观察者模式4、迭代子模式5、责任链模式6、命令模式7、备忘录模式8、状态模式9、访问者模式10、中介者模式11、解释器模式结构型模式处理类或对象间的组合。它将以不同的方式影响着程序，允许在补充写代码或自定义代码的情况下创建系统，而且具有重复使用性和应用性能结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。1、适配器模式2、装饰器模式3、代理模式4、外观模式5、桥接模式6、组合模式7、享元模式","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/设计模式/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/设计模式/"}]},{"title":"Docker的镜像","slug":"Docker的镜像","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-11T03:19:05.353Z","comments":true,"path":"2017/03/07/Docker的镜像/","link":"","permalink":"http://yoursite.com/2017/03/07/Docker的镜像/","excerpt":"","text":"子命令分类子命令Docker环境信息info、version容器生命周期管理create、exec、kill、pause、restart、rm、run、start、stop、unpause镜像仓库命令login、logout、pull、push、search镜像管理build、images、import、load、rmi、save、tag、commit容器运维操作attach、export、inspect、port、ps、rename、status、top、wait、cp、diff、update容器资源管理volume、network系统日志信息events、history、logsDocker的镜像拉取镜像Docker运行容器前需要本地存在对应的镜像，如果镜像不存在本地，Docker会从镜像仓库下载（默认是Docker Hub 公共注册服务器中的仓库）1docker pull [选项] [docker Registry地址]&lt;仓库名&gt;:&lt;标签&gt;Docker Registry地址：&lt;域名/IP&gt;[:端口号]，默认地址为Docker Hub仓库名：&lt;用户名&gt;/&lt;软件名&gt;。对于Docker Hub，默认为library，也就是官方的镜像。列出镜像1234567//-a 显示包括中间层镜像在内的全部镜像docker images [-a] [image_name]:[tag]//filter过滤参数 --filter/-fdocker images -f since=[image_name] #镜像版本之前的，用before#格式化显示docker images --format \"table &#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Repository&#125;&#125;\\t&#123;&#123;.Tag&#125;&#125;\"删除本地镜像12#镜像可以是 镜像短ID（去前3个字符以上）、镜像长ID、镜像名或者镜像摘要，(镜像名:Tag)docker rmi [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...]Commit定制镜像123//构建镜像，commit命令，不推荐//将容器的存储层保存下来成为镜像docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]]Dockerfile定制镜像基础命令1、FROM指定基础镜像2、 RUN执行命令shell格式：RUN &lt;命令&gt; ，直接在命令行输入命令1RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.htmlexec格式：RUN [“可执行文件”，”参数1”，”参数2”]，类似函数调用举个栗子：12345678910111213FROM debian:jessieRUN buildDeps='gcc libc6-dev make' \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y $buildDeps \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-3.2.5.tar.gz\" \\ &amp;&amp; mkdir -p /usr/src/redis \\ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ &amp;&amp; make -C /usr/src/redis \\ &amp;&amp; make -C /usr/src/redis install \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ &amp;&amp; rm redis.tar.gz \\ &amp;&amp; rm -r /usr/src/redis \\ &amp;&amp; apt-get purge -y --auto-remove $buildDepsRUN命令中使用&amp;&amp;将各个所需命令串联起来，支持Shell类 的行尾添加 \\ 的命令换行方式，以及行首#进行注释的格式在Dockerfile 文件所在目录执行，构建镜像。RUN指令首先启动一个容器，执行所要求的命令，提交一个新的镜像中，随后删除所用到的容器1docker build -t xx:v3 .镜像构建上下文在上面的命令中，我们发现在docker build的最后面有一个 . 表示当前目录。但是这不是在指定Dockerfile所在的路径。==因为在默认情况下，如果不指定Dockerfile，会将上下文目录下名为Dockerfile的文件作为Dockerfile。==再举个栗子吧：我们不能COPY ../xx/yyy 或者 COPY /opt/xx/yyy。因为这些路径已经是超出上下文的范围了，我们是将当前目录指定为上下文目录的。（Dockerfile文件名不需要为Dockerfile，也不要求位于上下文目录中，可以用 -f 来指定文件） 为什么我们需要上下文这个概念呢？因为docker客户端的命令通过Docker Remote API 和Docker引擎进行交互，一切都是使用远程调用形式在服务端（Docker引擎）完成。所以在我们构建的时候，用户会指定构建镜像上下文的路径。 3、COPY复制文件Docker的容器新建、启动容器1234567//新建并启动容器//-i标志保证容器中的STDIN开启，-t分配一个伪tty终端,-d为后台运行，//-p为指定端口映射，宿主机:容器，/bin/bash启动一个Bash shelldocker run --name yyy(容器名) [-d] -i -t xxxx(镜像名) /bin/bash//进入容器docker exec -it container_name /bin/bash12// -a 查看全部容器，包括停止和运行，-q 查看容器IDdocker ps -a1234567//启动已经停止的容器docker start container_name[或者container_id]//重启容器docker restart container_name[或者container_id]//删除容器docker rm [-f] docker rm $(docker ps -a -q)导入导出容器12345#导出容器快照docker export container_ID &gt; xxx.tar#导入容器快照cat xxx.tar | docker import - xxx/yyy:v1.0docker import urldocker load 和 docker import的区别：docker load 是导入镜像存储文件到本地镜像库。容器快照文件将丢弃所有的历史纪录和元数据信息（仅保存容器当时的快照状态），而镜像存储文件将保存完整纪录，体积也很大。容器互联容器的连接系统是除了端口映射外，另一种跟容器中应用交互的方式。该系统会在源和接受容器之间创建一个隧道，接受肉国企可以看到源容器指定的信息。使用–link参数使容器之间安全进行交互，–link name:alias，name为要链接的容器的名字，alias是这个链接的别名12345docker run -d --name db training/postgresdocker run -d -P --name web --link db:db training/webapp#用docker ps查看容器的连接在自定义命名的容器，有db和web，在db容器的names列有db,web/db，表示web容器链接到db容器，web容器被允许访问db容器信息。上面这个做法，可以避免db容器暴露数据库端口到外部网络上。1234#2种方式为容器公开连接信息第一种：环境变量，用env命令查看web容器的环境变量docker run --rm --name web2 --link db:db training/webapp env第二种：更新/etc/hosts文件Docker的仓库Docker Hub1234567#登录，输入用户名、密码、邮箱来完成注册和登录。注册完成后，本地用户目录.dockercfg保存用户的认证信息docker login #-s N：显示评价为N星以上的镜像docker search [-s N] image_name#下载镜像到本地docker pulldocker push私有仓库docker-registry是官方提供的工具，可以用于构建私有的镜像仓库。从容器运行Registry12#默认情况下，仓库会被创建在容器的/temp/registry下，-v指定将镜像文件存放在本地的指定路径docker run -d -p 5000:500 -v /opt/data/registry:/tmp/registry registry","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Spring Cloud（四）：熔断器Hystrix","slug":"Spring Cloud（四）：熔断器Hystrix","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-11T03:24:12.593Z","comments":true,"path":"2017/03/07/Spring Cloud（四）：熔断器Hystrix/","link":"","permalink":"http://yoursite.com/2017/03/07/Spring Cloud（四）：熔断器Hystrix/","excerpt":"","text":"什么是熔断器Hystrix？在分布式系统中，服务与服务之间的依赖错综复杂， 一种不可避免的情况就是某些服务会出现故障，导致依赖于它们的其他服务出现远程调度的线程阻塞。Hystrix 是Netflix 公司开源的一个项目，它提供了熔断器功能，能够阻止分布式系统中出现联动故障。Hystrix 是通过隔离服务的访问点阻止联动故障的，并提供了故障的解决方案，从而提高了整个分布式系统的弹性。出现问题的场景：由于服务的依赖性，会导致依赖于该故障服务的其他服务也处于线程阻塞状态，最终导致这些服务的线程资源消耗殆尽， 直到不可用，从而导致整个问服务系统都不可用，即雪崩效应。设计原则防止单个服务的故障耗尽整个服务的Servlet 容器（例如Tomcat ）的线程资源。快速失败机制，如果某个服务出现了故障，则调用该服务的请求快速失败，而不是线程等待。提供回退（ fallback ）方案，在请求发生故障时，提供设定好的回退方案。使用熔断机制，防止故障扩散到其他服务。提供熔断器的监控组件Hystrix Dashboard，可以实时监控熔断器的状态。工作机制创建HystrixCommand或HystrixObservableCommand对象命令行执行（命令行模式）HystrixCommand实现了execute（）同步执行、queue（）异步执行R value = command.execute();Future&lt;R&gt; fValue = command.queue();HystrixObservableCommand()实现了observe（）返回Hot Observable、toObservable（）返回Cold ObservableObservable&lt;R&gt; ohValue = command.observe();Observable&lt;R&gt; ohValue = command.toObservable();结果是否被缓存若当前命令的请求缓存功能是被启用，并且该命令缓存命中，那么缓存的结果会立即以Observable对象的形式返回。断路器是否打开在命令结果没有缓存命中时，Hystrix在执行命令前需要检查短路器是否为打开状态：断路器打开时，Hystrix不执行命令，转接到fallback处理逻辑，跳到第8步。断路器关闭时，Hustrix跳到第5步，检查是否有可用资源来执行命令线程池/请求队列/信号量是否占满如果线程池/请求队列/信号量都占满了，Hystrix不会执行命令，转接到fallback处理逻辑，跳到第8步。==注意：==这里的线程池是指每个依赖服务的专有线程池。Hystrix采用了“舱壁模式”来隔离每个依赖的服务HystrixObservableCommand.construct()或HystrixCommand.run()Hystrix根据我们编写的方法来决定采取什么的方式去请求依赖服务HystrixCommand.run()返回一个单一的结果，或者抛出异常HystrixObservableCommand.construct()返回一个Observable对象来发射多个结果或者通过onError发送错误通知计算断路器的健康度Hystrix会将“成功”、“失败”、“拒绝”、“超时”等信息报告给断路器，断路器维护一组计数器来统计这些数据。fallback处理（==服务降级==）需要实现一个通用的响应结果，并且该结果的处理逻辑是从缓存或者根据一些静态逻辑来获取，而不是依赖网络请求获取。（也可以包含网络请求，该请求要包装在HystrixCommand或HystrixObservableCommand中，从而形成了级联的降级策略，最终结果还是一个稳定的返回结果的处理逻辑）返回成功的响应依赖隔离优势点：应用自身得到完全保护，不会受不可控的依赖服务影响。即便给依赖服务分配的线程池被填满，也不会影响应用自身的其余部分可以有效降低接入新服务的风险。如果新服务接入后运行不稳定或存在问题，完全不会影响应用其他的请求。依赖的服务从失效恢复正常后，它的线程池会被清理并能够马上恢复健康的服务。依赖的服务出现配置错误的时候，线程池会快速反映出问题每个专有线程池提供内置的并发实现Hystrix接口和注解Hystrix DashBoard提供了数据监控和友好的图形化展示界面支持三种不同的监控方式：默认的集群监控：通过URL http://turbine-hostname:port/turbine.stream开启，实现对默认集群的监控指定的集群监控：通过URL http://turbine-hostname:port/turbine.stream?cluster=[clusterName]开启，实现对clusterName集群的监控单体应用的监控：通过URL http://turbine-hostname:port/hystrix.stream开启，实现对具体单个服务实例的监控如果是监控多个集群应用，方法有点不一样，主要是消费者的配置文件要加点东西。跟着我看一下吧turbine配置文件：1234567turbine: aggregator: cluster-config: mycluster #自定义集群名称 app-config: eureka-feign-client #,eureka-ribbon-client cluster-name-expression: metadata['cluster'] #固定的 combine-host-port: true instanceUrlSuffix: /hystrix.stream消费者的配置文件：123instance: metadata-map: cluster: myclusterTurbine监控用这个Turbine的想法很简单，一个词叫做聚合。把多实例放在一起来统计、管理、监控pom.xml1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-turbine&lt;/artifactId&gt;&lt;/dependency&gt;在启动类上面加注解@EnableTurbine开启Turbine配置文件1234567turbine: aggregator: cluster-config: default app-config: eureka-ribbon-client,eureka-feign-client cluster-name-expression: new String(\"default\") combine-host-port: true instanceUrlSuffix: /hystrix.streamaggregator.cluster-config：指定聚合哪些集群，多个使用”,”分割，默认为default。可使用http://…/turbine.stream?cluster={clusterConfig之一}访问app-config：需要收集监控信息的服务名cluster-name-expression：指定集群名称为default，如果为default时，turbine.aggregator.clusterConfig可以不写，因为默认就是defaultcombine-host-port：设置为true，可以让一台主机上的服务通过主机名和端口号组合来区分。默认是通过host来区分不同的服务的。启动Turbine重启工程一切正常，打开 http://localhost:port/hystrix 然后输入监控地址，因为前面我们使用的是default cluster所以这里输入：http://localhost:port/turbine.stream点击 Monitor Stream 按钮后，需要访问 http://localhost:8764/testRibbon?name=rocye 和 http://localhost:port/testRibbon?name=rocye 可以多刷新几次，可以看到如下效果仪表盘参数详细说明Turbine Stream遇到的问题：Unable to connect to Command Metric Stream解决方案：12345678910111213@Configurationpublic class DashBoardConfig &#123; @Bean public ServletRegistrationBean servletRegistrationBean() &#123; HystrixMetricsStreamServlet streamServlet = new HystrixMetricsStreamServlet(); ServletRegistrationBean registrationBean = new ServletRegistrationBean(streamServlet); registrationBean.setLoadOnStartup(1); registrationBean.addUrlMappings(\"/hystrix.stream\"); registrationBean.setName(\"HystrixMetricsStreamServlet\"); return registrationBean; &#125;&#125;Turbine404问题123&gt; com.netflix.turbine.monitor.instance.InstanceMonitor$MisconfiguredHostException:&gt; [&#123;&quot;timestamp&quot;:&quot;2018-06-28T08:01:38.908+0000&quot;,&quot;status&quot;:404,&quot;error&quot;:&quot;Not Found&quot;,&quot;message&quot;:&quot;No message available&quot;,&quot;path&quot;:&quot;/actuator/hystrix.stream&quot;&#125;]&gt;解决办法：1234567turbine: #aggregator: #cluster-config: default app-config: eureka-feign-client #,eureka-ribbon-client cluster-name-expression: new String(\"default\") #metadata['cluster'] combine-host-port: true instanceUrlSuffix: /hystrix.stream #一定要加这个","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"CommonJS 详细介绍","slug":"commonjs","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-10T02:13:19.495Z","comments":true,"path":"2017/03/07/commonjs/","link":"","permalink":"http://yoursite.com/2017/03/07/commonjs/","excerpt":"CommonJS 规范与实现正如当年为了统一 JavaScript 语言标准，人们制定了 ECMAScript 规范一样，如今为了统一 JavaScript 在浏览器之外的实现，CommonJS 诞生了。CommonJS 试图定义一套普通应用程序使用的 API，从而填补 JavaScript 标准库过于简单的不足。CommonJS 的终极目标是制定一个像 C++ 标准库一样的规范，使得基于 CommonJS API 的应用程序可以在不同的环境下运行，就像用 C++ 编写的应用程序可以使用不同的编译器和运行时函数库一样。为了保持中立，CommonJS 不参与标准库实现，其实现交给像 Node.js 之类的项目来完成。下图是 CommonJS 的各种实现。","text":"CommonJS 规范与实现正如当年为了统一 JavaScript 语言标准，人们制定了 ECMAScript 规范一样，如今为了统一 JavaScript 在浏览器之外的实现，CommonJS 诞生了。CommonJS 试图定义一套普通应用程序使用的 API，从而填补 JavaScript 标准库过于简单的不足。CommonJS 的终极目标是制定一个像 C++ 标准库一样的规范，使得基于 CommonJS API 的应用程序可以在不同的环境下运行，就像用 C++ 编写的应用程序可以使用不同的编译器和运行时函数库一样。为了保持中立，CommonJS 不参与标准库实现，其实现交给像 Node.js 之类的项目来完成。下图是 CommonJS 的各种实现。CommonJS 规范包括了模块（modules）、包（packages）、系统（system）、二进制（binary）、控制台（console）、编码（encodings）、文件系统（filesystems）、套接字（sockets）、单元测试（unit testing）等部分。Node.js 是目前 CommonJS 规范最热门的一个实现，它基于 CommonJS 的 Modules/1.0 规范实现了 Node.js 的模块，同时随着 CommonJS 规范的更新，Node.js 也在不断跟进。模块（Module）和包（Package）是 Node.js 最重要的支柱。开发一个具有一定规模的程序不可能只用一个文件，通常需要把各个功能拆分、封装，然后组合起来，模块正式为了实现这种方式而诞生的。在浏览器 JavaScript 中，脚本模块的拆分和组合通常使用 HTML 的 script 标签来实现。Node.js 提供了 require 函数来调用其他模块，而且模块都是基于文件的，机制十分简单。CommonJS 规范的实现Node.js 的模块和包机制的实现参照了 CommonJS 的标准，但并未完全遵循。不过两者的区别不大，一般来说你大可不必担心，只有当你试图制作一个除了支持 Node.js 之外还要支持其他平台的模块或包的时候才需要仔细研究。通常，两者没有直接冲突的地方。我们经常把 Node.js 的模块和包相提并论，因为模块和包是没有本质区别的，两个概念也时常混用。如果要辨析，那么可以把包理解成是实现了某个功能模块的集合，用于发布和维护。对使用者来说，模块和包的区别是透明的，因此经常不作区分。CommonJS 规范规定，每个模块内部，module 变量代表当前模块。这个变量是一个对象，它的 exports 属性（即 module.exports）是对外的接口。加载某个模块，其实是加载该模块的 module.exports 属性。为了方便，Node.js 为每个模块提供一个 exports 变量，指向 module.exports。这等同在每个模块头部，有一行这样的命令：1var exports = module.exports;注意，不能直接将 exports 变量指向一个值，因为这样等于切断了 exports 与 module.exports 的联系。如果你觉得，exports 与 module.exports 之间的区别很难分清，一个简单的处理办法，就是放弃使用 exports，只使用 module.exports。什么是模块模块是 Node.js 应用程序的基本组成部分，文件和模块是一一对应的。换言之，一个 Node.js 文件就是一个模块，这个文件可能是 JavaScript 代码、JSON 或者编译过的 C/C++ 扩展。创建及加载模块创建模块在 Node.js 中，创建一个模块非常简单，因为一个文件就是一个模块，我们要关注的问题仅仅在于如何在其他文件中获取这个模块。Node.js 提供了 exports 和 require 两个对象，其中 exports 是模块公开的接口，require 用于从外部获取一个模块的接口，即所获取模块的 exports 对象。让我以一个例子来了解模块。创建一个 module.js 文件，内容是：12345678// module.jsvar name;exports.setName = function(thyName) &#123; name = thyName;&#125;;exports.sayHello = function() &#123; console.log('Hello ' + name);&#125;;在同一目录下创建 getmodule.js，内容是：1234// getmodule.jsvar myModule = require('./module');myModule.setName('Yu');myModule.sayHello();运行 node getmodule.js ，结果是：Hello Yumodule.js 通过 exports 对象把 setName 和 sayHello 作为模块的访问接口，在 getmodule.js 中通过 require(&#39;./module&#39;) 加载这个模块，然后就可以直接访问 module.js 中 exports 对象的成员函数了。加载模块在 Node.js 中，我们可以直接通过 require 获取核心模块，例如 require(&#39;fs&#39;) 。核心模块拥有最高的加载优先级，换言之如果有模块与其命名冲突，Node.js 总是会加载核心模块。如果有模块与核心模块命名冲突，Node.js 为什么可以选择加载核心模块呢？require 的实现机制是怎样的呢？1、按路径加载模块如果 require 参数以 “/“ 开头，那么就以绝对路径的方式查找模块名称，例如 require(&#39;/home/neveryu/module&#39;) 将会按照 优先级依次尝试加载 /home/neveryu/module.js、/home/neveryu/module.json 和 /home/neveryu/module.node。如果 require 参数 “./“ 或 “../“ 开头，那么则以相对路径的方式查找模块，这种方式在应用中是最常见的。例如前面的例子中我们用了 require(&#39;./hello&#39;)来加载同一文件夹下的 hello.js。2、通过查找 node_modules 目录加载模块如果 require 参数不以 “/“ ， “./“ 或 “../“ 开头，而该模块又不是核心模块，那么就要通过查找 node_modules 加载模块了。我们使用 npm 获取的包通常就是以这种方式加载的。在 node_modules 目录的外面一层，外面可以直接使用 require(&#39;express&#39;) 来代替 require(&#39;./node_modules/express&#39;)。这是 Node.js 模块加载的一个重要特征：通过查找 node_modules 目录来加载模块。我们不仅要在 project 目录下的 app.js 中使用 require(&#39;express&#39;)，而且可能要在 controllers 子目录下的 index_controller.js 中也使用 require(&#39;express&#39;)，这时就需要向父目录上溯一层才能找到 node_modules 中的 express 了。3、加载缓存Node.js 通过文件名缓存所有加载过的文件模块，所以以后再访问到时就不会重新加载了。注意，Node.js 是根据实际文件名缓存的，而不是 require() 提供的参数缓存的，也就是说即使你分别通过 require(&#39;express&#39;) 和 require(&#39;./node_modules/express&#39;)加载两次，也不会重复加载，因为尽管两次参数不同，解析到的文件却是同一个。单次加载上面这个例子有点类似于创建一个对象，但实际上和对象又有本质的区别，因为 require 不会重复加载模块，也就是说无论调用多少次 require，获得的模块都是同一个。我们在 getmodule.js 的基础上稍作修改：12345678// loadmodule.jsvar hello1 = require('./module');hello1.setName('Yu');var hello2 = require('./module');hello2.setName('Yu 2');hello1.sayHello();运行后发现输出结果是 Hello Yu 2，这是因为变量 hello1 和 hello2 指向的是同一个实例，因此 hello1.setName 的结果被 hello2.setName 覆盖，最终输出结果是由后者决定的。覆盖 exports有时候我们只是想把一个对象封装到模块中，例如：123456789101112// singleobjct.jsfunction Hello() &#123; var name; this.setName = function (thyName) &#123; name = thyName; &#125;; this.sayHello = function () &#123; console.log('Hello ' + name); &#125;;&#125;exports.Hello = Hello;此时我们在其他文件中需要通过 require(&#39;./singleobject&#39;).Hello 来获取 Hello 对象，这略显冗余，可以用下面方法稍微简化。1234567891011// hello.jsfunction Hello() &#123; var name; this.setName = function(thyName) &#123; name = thyName; &#125;; this.sayHello = function() &#123; console.log('Hello ' + name); &#125;;&#125;module.exports = Hello;这样就可以直接获得这个对象了：12345// gethello.jsvar Hello = require('./hello');hello = new Hello();hello.setName('Yu');hello.sayHello();TipCommonJS 模块的特点如下：所有代码都运行在模块作用域，不会污染全局作用域。独立性是模块的重要特点就，模块内部最好不与程序的其他部分直接交互。模块可以多次加载，但是只会在第一次加载时运行一次，然后运行结果就被缓存了，以后再加载，就直接读取缓存结果。要想让模块再次运行，必须清除缓存。模块加载的顺序，按照其在代码中出现的顺序。CommonJS 中的 Require建议阅览：CommonJS require 规范","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/前端/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"}]},{"title":"Docker数据管理","slug":"Docker数据管理","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-11T03:19:05.330Z","comments":true,"path":"2017/03/07/Docker数据管理/","link":"","permalink":"http://yoursite.com/2017/03/07/Docker数据管理/","excerpt":"","text":"Docker数据管理数据卷数据卷是一个或多个容器专门指定绕过Union File System的特殊目录，为持续性或共享数据提供一些有用的功能。数据卷可以用来存储Docker应用的数据，也可以用来在Docker容器间进行数据共享。 使用Docker的数据卷，类似在系统中使用 mount 挂载一个文件系统数据卷可以在容器间共享和重用数据卷数据改变是直接修改的对数据卷的更新，不会影响镜像数据卷是持续性的，直到没有容器使用它们添加一个数据卷使用 -v 选项添加一个数据卷，或者可以使用多次 -v 选项为一个 docker 容器运行挂载多个数据卷12#加载数据卷到容器的/webapp目录sudo docker run -d -P --name web -v /webapp training/webapp python app.py删除数据卷数据卷是独立于容器，Docker不会在容器被删除后自动删除数据卷，也不会有垃圾回收机制来处理。如果要删除数据卷，可以在删除容器的时候，加个 -v 的选项。挂载一个主机目录作为数据卷挂载是啥意思啊？答：挂载就是把设备放在一个目录下，用U盘例子理解一下哦~可以直接挂载宿主机文件或目录(设备)到容器(目录)里，可以理解为目录映射，这样就可以让所有的容器共享宿主机数据，从而只需要改变宿主机的数据源就能够影响到所有的容器数据。挂载的数据默认为可读写权限。 :ro 为挂载的数据为只读123#-v后面的映射关系是\"宿主机文件/目录:容器里对应的文件/目录\"，其中，宿主机上的文件/目录是要提前存在的，容器里对应的文件/目录会自动创建。 docker run -t -i --name test -v /src/webapp/1.txt:/opt/webapp/1.txt:ro docker.io/centos /bin/bashdocker run -t -i --name hqsb -v /src/webapp:/opt/webapp docker.io/centos /bin/bash查看数据卷的具体信息1docker inspect web数据卷容器数据卷容器，其实是一个正常的容器，专门用来提供数据卷供其它容器挂载的。用户需要在多个容器之间共享一些持续更新的数据，最好用数据卷容器123456#创建数据卷容器docker run -d -v /dbdata --name dbdata training/postgres#使用 --volumes-from 来挂载容器中的数据卷docker run -d --volumes-from dbdata --name db1 training/postgresdocker run -d --name db3 --volumes-from db1 training/postgres利用数据卷容器来迁移数据可以利用数据卷容器对其中的数据卷进行备份、恢复，以实现数据的迁移。备份首先利用ubuntu镜像创建了一个容器worker。使用–volumes-from dbdata参数来让worker容器挂载dbdata容器的数据卷(即dbdata数据卷),使用-v $(pwd):/backup参数来挂载本地的当前目录到worker容器的/backup目录。worker容器启动后，使用了tar cvf /backup/backup.tar /dbdata命令来将/dbdata下内容备份为容器内的/backup/backup.tar，即宿主主机当前目录下的backup.tar。1docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata恢复首先创建一个带有数据卷的容器dbdata2：1docker run -v /dbdata --name dbdata2 ubuntu /bin/bash然后创建另一个新的容器，挂载dbdata2的容器，并使用untar解压备份文件到所挂载的容器卷中：1docker run --volumes-from dbdata2 -v $(pwd):/backup --name worker ubuntu tar xvf /backup/backup.tar","categories":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"Spring Cloud（三）：负载均衡Ribbon","slug":"Spring Cloud（三）：负载均衡Ribbon","date":"2017-03-07T05:25:24.000Z","updated":"2019-04-11T03:25:10.791Z","comments":true,"path":"2017/03/07/Spring Cloud（三）：负载均衡Ribbon/","link":"","permalink":"http://yoursite.com/2017/03/07/Spring Cloud（三）：负载均衡Ribbon/","excerpt":"","text":"什么是Spring Cloud Ribbon?Spring Cloud Ribbon 是一个基于HTTP 和 TCP 的客户端负载均衡工具，它基于Netflix Ribbon实现的。通过Spring Cloud的封装，让我们轻松地将面向服务的REST模板请求自动转换成客户端负载均衡的服务调用。Spring cloud有两种服务调用方式，一种是Ribbon+restTemplate，另一种是feign。Feign，也是基于Ribbon实现的工具。客户端负载均衡和服务器负载均衡的区别？服务端负载均衡：分为硬件负载均衡和软件负载均衡。硬件负载均衡主要是通过在服务器节点之间安装专门用于负载均衡的设备，如F5等；软件负载均衡是通过在服务器上安装一些具有负载均衡或者模块的软件来完成请求分发工作，如nginx。它们都会维护一个下挂可用的服务端清单，通过心跳检测来剔除故障的服务端节点以保证清单中都是可以正常访问的服务端节点。当客户端发送请求给负载均衡的设备时，该设备按照某种算法从维护的可用服务端清单中取一台服务端地址，然后进行转发。客户端负载均衡：基于客户端的负载均衡，客户端会有一个服务器地址列表。简单的说就是在客户端程序里面，自己设定一个调度算法，在向服务器发起请求的时候，先执行调度算法计算出向哪台服务器发起请求，然后再发起请求给服务器。RestTemplate详解代码配置：123456//该对象会使用Ribbon的自动化配置@Bean@LoadBalanced //开启客户端负载均衡RestTemplate restTemplate()&#123; return new RestTemplate();&#125;1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId &gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt;请求类型：GET、POST、PUT、DELETE、HEAD请求负载均衡器LoadBalancerAutoConfiguration为实现客户端负载均衡器的自动化配置类。什么是Feign?Feign是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用Feign 注解和JAX-RS注解。Feign支持可插拔的编码器和解码器。Feign默认集成了Ribbon，并和Eureka结合，默认实现了负载均衡的效果。只需要创建一个接口并用注解的方式来配置它，即可对服务提供方的接口的绑定12345&lt;!-- Finchley.SR1版本 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;123@SpringBootApplication@EnableFeignClientspublic class EurekaFeignClientApplication &#123;12345678//@ FeignClient（“服务名”），来指定调用哪个服务//如：调用了service-hi服务的“/hi”接口@FeignClient(value = \"service-hi\")public interface SchedualServiceHi &#123; //使用了spring mvc的注解 @RequestMapping(value = \"/hi\",method = RequestMethod.GET) String sayHiFromClientOne(@RequestParam(value = \"name\") String name);&#125;源码分析@FeignClient注解用于创建声明式API接口，该接口是RESTful风格的。value()和name()是一样的，是被调用的服务的ServiceId，url()直接填写硬编码的Url地址。configuration()指明FeignClient的配置类，默认的配置类为FeignClientsConfiguration类。工作原理首先通过@EnableFeignClients注解开启FeignClient的功能。只有这个注解的存在，才能在程序启动时开启对@FeignClient注解的包扫描根据Feign的规则实现接口，并在接口上加上@FeignClient注解程序启动后，会进行包扫描，扫描所有的@FeignClient注解的类，并将这些信息注入Ioc容器当接口的方法被调用时， 通过JDK 的代理来生成具体的RequestTe m p l ate 棋根对象。根据RequestTemplate 再生成Http 请求的Request 对象。Request 对象交给Client 去处理， 其中Client 的网络请求框架可以是HttpURLConnection、HttpClient 和OkHttp 。最后Client 被封装到LoadBalanceClient 类，这个类结合类Ribbon 做到了负载均衡。","categories":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/categories/Spring-Cloud/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://yoursite.com/tags/Spring-Cloud/"}]},{"title":"中国国内 JavaScript 圈的现状如何","slug":"china-javascript-environment","date":"2016-11-18T05:25:24.000Z","updated":"2018-10-20T07:46:51.000Z","comments":false,"path":"2016/11/18/china-javascript-environment/","link":"","permalink":"http://yoursite.com/2016/11/18/china-javascript-environment/","excerpt":"探讨核心： 我们不编写代码，我们只是国外优秀框架的搬运工。","text":"探讨核心： 我们不编写代码，我们只是国外优秀框架的搬运工。现状么二五八万忙着造轮子步道小白忙着加群拜山头还有几个默默钻研又不为人知的浮躁，自我，没劲有天 leader 抽烟吹逼时候说：国内这些前端啊，多数靠的是名声，写点东西造个轮子，参加一下活动吹吹逼，建个群拉个山头搞一帮小弟，等公司挖了当上leader就算升天了国内 star 最多的没记错是NW吧，噢，人家是C艹分类下的玻璃心洗洗睡吧 只不过很多开发者停留在外来的和尚会念经阶段。如果很多老外都在用你的技术，国内的名博上写上一篇你的文章表扬一下，国内脑残粉扎堆严重。放大些来看，不只前端，其他行业的观念也是如此。另外，国内开源心态还不成熟，拿来主义倾向严重。遇到问题，你不帮 ta 解决问题你做的技术就是无用论的不在少数。使得很多开发人员认为做这样的开源简直就是做雷锋，没什么意义，所以很多牛人独善其身的不少。认为开源的技术就是无 bug ，产品，是完美，否则你就不要开，这种扯蛋的心态的人也很多。有个东西叫KPI的好吗，你成天在公司，不折腾点东西怎么算KPI，怎么分奖金?为了自身业务和KPI需求，在大点的公司，是肯定有自己的框架的,虽然往往里面充满了来自jquery啊prototypejs等等类库或者框架的几乎一样的代码，甚至在自身需求上还剪掉了一些通用方法，用开发者的话说，这个太臃肿了，我的简洁，然后慢慢的，功能加上来，文件也越来越大了，最后也都趋于跟其他已经使用广泛的类库或者框架一致…在量的堆积下，国内的开源东西也凸显了不少好用的，比如fis、eccharts、sea、至于kissy之流咱就忽略吧，毕竟可能是自己的业务需求嘛，虽然文档写的好烂,领导升职了，后续也没人搞了….另外还有一个问题，国内很多框架什么的，核心就一个人，在工作繁忙的时候，bug也就没有精力去修复,框架能存在多久，是个问题…有时候在群里都有人宣传自己的框架的，说不定一两年后，人都转行了，之前的框架呢？会有人接手？悄无声息死掉的框架，大把的。确实搬运了不少国外牛人的框架，但是我也要说一句吐槽的话【bug真特么的多，屁股还要自己擦】水平不够只有搬运，擦屁股才是个技术活。国内其实是有不少在做框架开发或者库开发的，包括BAT新浪网易等等，只是这些框架多数主要围绕自身业务做的，在加上文档、封闭性等等原因其它公司不太适合去用。比如当年做的还不错的seajs，从前端的设计思路和实现上都很简洁明了，只是当时它们要支持自己的复杂业务搞了一套繁琐的路径配置逻辑结果就让人比较头疼了。所以我觉得最重要的原因还是环境问题，没有包容、协作、共赢的生态环境人们的思维就会局限在谋求自身的利益，而不是整体价值的最大化。我倒是觉得做搬运工也没什么丢人的，只能说我们条件还不具备，能给出一些有用的反馈也算是有价值的事情了。开源不分国界，私以为在这个时代还纠结于国内外，纠结于自主知识产权，纠结于国人当自强的都是莫名其妙的民族主义在作祟。这不是民族主义，这是圈子问题。你的圈子里没有牛人能做出这些东西，这就是差距。国内圈子要是和湾区的交流跟加拿大人一样方便频繁肯定没人说这话。我想问的是那些自己发明轮子的同行们，Backbone.js、AngularJS、jQuery、RequireJS、Bower、Grunt、Yomen 等等无数的开源框架和库以及整个开源社区组成的前端开发生态圈的各种免费解决方案们，到底无法解决工作当中的哪些实际问题，以至于要不停的重复发明轮子，而且还是方的轮子，难道只是为了秀肌肉？或者在各种场合做 talk？国内某圈子都是国外技术的搬运工不可怕，可怕的是几个高票答案里那股反智主义倾向。有优秀的轮子为什么不拿来用？实际上这并不只是国内JS圈的情况，甚至不只是JS圈的情况，其实世界上大多数程序员都是优秀的搬运工啊XD。谢邀。我工作中并不用js，所以对于js圈的话并不是很熟悉。题主说国内只是优秀库的搬运工，其实对于这句话我觉得应该客观对待，确实很多前端都是在使用国外的优秀库，比如boostrap等，但是会用与能用是两个不同的概念，会用意味着肯定是有pr的，会用的人水平也应该是肯定的。而且国内也有非常多的优秀js库，比如老赵的wind.js，淘宝的kissy等，再看诸如淘宝，腾讯，360等前端团队牛人泛多，他们是绝对有能力写出题主所定义的优秀库的，有能力会用而且会改造现有的优秀库其实力也不是一般的。所以对于是否是国外优秀库的搬运工，这里应该不存在绝对的定论，理性看待这个问题，至少鄙人非常乐观。其实我刚入行的时候，真的很认真的去听所有国内的 talk，想听到他们为什么做这样的库、做那样的框架，而不是使用开源的解决方案，目前为止，没听到有说服力的理由。很多答案都提到了国内的几个优秀开源库，当然前端圈子里面优秀的开源库很多，但目之所及，更多的是照猫画虎反类犬，至于哪些经典案例，我还是怕得罪同行的，就不说了。我真的希望能和同行们一起做点别人没做过的库和框架，解决一些现在没人解决的问题，改善一下这种浮躁的风气。所以这个评价算是很中肯了，至少我会选择更务实的同事一起工作。我匿名是怕得罪同行，毕竟还要混饭吃。但是替他们说话的人还匿名恐怕是中枪了吧？Gulp 和 Grunt 用不同的方式解决同一个问题，并且各有千秋，但请别告诉我你们发明的轮子比人家的好用。没有人会反对创新和自主创造，反对的是用同一个姿势吃人家剩下的东西。就算是同样的东西，你能说出 Bootstrap 和 GWSK 哪个更好吗，恐怕是同样好吧？但请别说 jQuery 和 XX 板，RequireJS 和 XXXJS 一样好用。你当然也可以反驳说如果不做永远都无法超越，那么首先请你做的至少和人家一样好了以后再到处去布道去秀，另外我始终不觉得超越是什么伟大到值得拿出来说的目的。作为一个工程师，我只想做出真正有用、用户喜欢的产品，对于我所用的框架到底是中国人开发的还是外国人开发的，对不起我不 care，如果你说这叫反智我只能呵呵了。我对这么多答案中表现出来的对于浪费公司资源去做完全无意义的事情然后还到处去显摆并作为 promotion 资本这件事情表现出来的漠然感到震惊。这个世界上从来都不缺想让世界变得更好的人，特别是工程师群体，但可惜的是出于某些邪恶目的而重复发明轮子的人也不少。我不否定国内有优秀的轮子，并且敬佩能做出好的开源项目的人，但我敬佩你不会是因为你有个「中国工程师」的定语。要强调的的是国内前端圈子里面非常流行的「为了做而做，做的是完全一样的东西还没人家好，然后还到处去显摆」这种行为真的是令人恶心。排名第一的答案的外链中有一句话：“我以前就一直说：我们离最先进的技术的差距只有半年。”其实这已经很说明问题了，离最先进的技术有差距（不管是半年还是半个月），这说明国内做的东西不是原创的，而是在重复造轮子（有些有微创新，当然我相信也有原创的，但应该非常少）比如 ueditor，国外都有那么强大的编辑器了，如果照着它都不能造一个轮子出来，那只能怀疑造轮子的人水平有问题。我记得 ueditor 团队还制作了个视频宣讲 ueditor，其中说到xxx还用了很nb的算法（二分查找），不知道的人还真以为是自己想出来的，回来一看 fckeditor，代码都几乎一模一样（当然 ueditor 本身是很不错的，比较符合国情）。再说 SeaJS，CMD 是神马啊，它真的很C（ommon）吗？SeaJS 模块难道不是异步模块（AMD）吗？命名上就把人搞晕了。怎么解释还需要修改 jQuery 源代码的问题？jQuery 会来兼容 SeaJS 吗？你们知道有多少人兴致勃勃地用了 SeaJS 后最后又切换到 requierJS 吗？再说下，国内的技术人员不但不团结，还互相讥讽，我相信这一点玉伯深有体会。大家可以去看下 SeaJS 的源代码贡献者，除了玉伯你们都还认识谁？SeaJS 团队发布了 SPM 工具，遭受了多少的嘲笑啊！虽然 SPM 最为人诟病。整天张口闭口说开源的那些人，怎么都觉得 SeaJS 已经很完美了？不是不用国内的东西，而是用过后觉得很失望，要文档没文档，说改就改不够稳定。并且据我所知，很多项目的核心开发就一个人，我能相信这东西靠谱吗？我可要为我整个公司负责啊，解决业务需求是我的首要目标，出了 bug 我可担当不起啊。重复造轮子没问题，但你造个方的轮子或者和不符合国际标准的轮子还到处得瑟让你的同事或者业界朋友来用，你就是来捣乱的！语气重了点，希望大家见谅！中小公司缺乏强力统一的方向和规范，很多都只是搬运工，没有深入。大公司如bat之类的会有自己的体系，也是众多工程师的学习楷模。原文","categories":[{"name":"综合","slug":"综合","permalink":"http://yoursite.com/categories/综合/"}],"tags":[{"name":"essay","slug":"essay","permalink":"http://yoursite.com/tags/essay/"},{"name":"JavaScript","slug":"JavaScript","permalink":"http://yoursite.com/tags/JavaScript/"}]},{"title":"Git学习总结","slug":"git","date":"2016-10-07T05:25:24.000Z","updated":"2018-10-20T07:46:51.000Z","comments":false,"path":"2016/10/07/git/","link":"","permalink":"http://yoursite.com/2016/10/07/git/","excerpt":"git 简介git 是分布式的，所以其核心就是分支，分支的意义在于，可以将项目代码按照功能、模块拆分成不同的分支。比如这个产品要加一个支付功能和一个登陆功能，可以创建两个分支，交给不同的开发人员并行开发。登陆功能先开发完，测试无误后合并改分支到 master 分支，master 分支部署上线。支付功能虽然没有开发完成，但是在另一条分支上，所以产品上线和功能开发完全不受影响。这才是分布式开发的高效模式。在 git 中，工作目录下面的所有文件都不外乎这两种状态：已跟踪或未跟踪。已跟踪的文件是指本来就被纳入版本控制管理的文件，在上次快照中有它们的记录，工作一段时间后，它们的状态可能是未更新，已修改或者已放入暂存区。而所有其他文件都属于未跟踪文件。它们既没有上次更新时的快照，也不在当前的暂存区域。初次克隆某个仓库时，工作目录中的所有文件都属于已跟踪文件，且状态为未修改。","text":"git 简介git 是分布式的，所以其核心就是分支，分支的意义在于，可以将项目代码按照功能、模块拆分成不同的分支。比如这个产品要加一个支付功能和一个登陆功能，可以创建两个分支，交给不同的开发人员并行开发。登陆功能先开发完，测试无误后合并改分支到 master 分支，master 分支部署上线。支付功能虽然没有开发完成，但是在另一条分支上，所以产品上线和功能开发完全不受影响。这才是分布式开发的高效模式。在 git 中，工作目录下面的所有文件都不外乎这两种状态：已跟踪或未跟踪。已跟踪的文件是指本来就被纳入版本控制管理的文件，在上次快照中有它们的记录，工作一段时间后，它们的状态可能是未更新，已修改或者已放入暂存区。而所有其他文件都属于未跟踪文件。它们既没有上次更新时的快照，也不在当前的暂存区域。初次克隆某个仓库时，工作目录中的所有文件都属于已跟踪文件，且状态为未修改。实用指令详解merge通常，合并分支时，如果可能，Git 会用 Fast froward 模式，但这种模式下，删除分支后，会丢掉分支信息。如果要强制禁用 Fast forward 模式，Git 就会在 merge 时生成一个新的 commit ，这样，从分支历史上就可以看出分支信息。git merge --no-ff -m &#39;merge with no-ff&#39; dev因为本次合并要创建一个新的 commit，所以加上 -m 参数，把 commit 描述写进去。合并分支时，加上 --no-ff 参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而 fast forward 合并就看不出来曾经做过合并。fetch一旦远程主机的版本库有了更新(git 术语叫做 commit)，需要将这些更新取回本地，这时就要用到 git fetch 命令。git fetch &lt;远程主机名&gt;上面命令将某个远程主机的更新，全部取回本地。默认情况下，git fetch 取回所有分支(branch)的更新。如果只想取回特定分支的更新，可以指定分支名。git fetch &lt;远程主机名&gt; &lt;分支名&gt;比如，取回 origin 主机的 master 分支git fetch origin master所取回的更新，在本地主机上要用“远程主机名/分支名”的形式读取。比如 origin 主机的 master，就要用 origin/master 读取。git fetch -p ：取回远程更新，删除不存在的分支。pullgit pull 命令的作用是，取回远程主机的某个分支的更新，再与本地的指定分支合并，完整格式如下：git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;比如，取回origin主机的next分支，与本地的master分支合并，需要写成下面这样。git pull origin next:master如果远程分支是与当前分支合并，则冒号后面的部分可以省略。git pull origin master上面的命令表示，取回 origin/master 分支，再与当前分支合并。实质上，这等同于先做 git fetch，再 merge。12git fetch origingit merge origin/master在某些场合，git 会自动在本地分支与远程分支之间，建立一种追踪关系(tracking)。比如，在 git clone 的时候，所有本地分支默认与远程主机的同名分支，建立追踪关系，也就是说，本地的 master 分支自动“追踪” origin/master 分支。git 也允许手动建立追踪关系。git branch --set-upstream master origin/next上面的命令指定 master 分支追踪 origin/next 分支。如果当前分支与远程分支存在追踪关系，git pull 就可以省略远程分支名。git pull origin上面命令表示，本地的当前分支自动与对应的 origin 主机“追踪分支”(remote-tracking branch)进行合并。如果当前分支只有一个追踪分支，连远程主机名都可以忽略。git pull上面命令表示，当前分支自动与唯一一个追踪分支进行合并。如果合并需要采用 rebase 模式，可以使用 -rebase 选项。git pull --rebase &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;pushgit push 命令用于将本地分支的更新，推送到远程主机。它的格式与 git pull 命令相仿。git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;如果省略远程分支名，则表示将本地分支推送与之存在“追踪关系”的远程分支(通常两者同名)，如果该远程分支不存在，则会被新建。git push origin master上面命令表示，将本地的 master 分支推送到 origin 主机的 master 分支。如果后者不存在，则会被新建。如果省略本地分支名，则表示删除指定的远程分支，因为这等同于推送一个空的本地分支到远程分支。git push origin :master等同于git push origin --delete master上面命令表示删除 origin 主机的 master 分支。如果当前分支与远程分支之间存在追踪关系，则本地分支和远程分支都可以省略。git push origin上面命令表示，将当前分支推送到 origin 主机的对应分支。如果当前分支只有一个追踪分支，那么主机名都可以省略。git push如果当前分支与多个主机存在追踪关系，则可以使用-u选项指定一个默认主机，这样后面就可以不加任何参数使用 git push 。git push -u origin master上面的命令将本地 master 分支推送到 origin 主机，同时指定 origin 为默认主机，后面就可以不加任何参数使用 git push 了。不带任何参数的 git push ，默认只推送当前分支，这叫做 simple 方式。此外，还有一种 matching 方式，会推送所有有对应的远程分支的本地分支。git 2.0 版本之前，默认采用 matching 方式，现在改为默认采用 simple 方式，如果要修改这个设置，可以采用 git config 命令。git config --global push.default matching或者git config --global push.default simple还有一种情况，就是不管是否存在对应的远程分支，将本地的所有分支都推送到远程主机，这时需要使用 -all 选项。git push --all origin上面命令表示，将所有本地分支都推送到 origin 主机。如果远程主机的版本比本地版本更新，推送时 git 会报错，要求先在本地做 git pull 合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用 -force 选项。git push --force origin上面命令使用-force选项，结果导致在远程主机产生一个“非直进式”的合并(non-fast-forward merge)。除非你很确定要这样做，否则应该尽量避免使用 -force 选项。最后，git push 不会推送标签(tag)，除非使用 -tags 选项。git push origin --tagsloggit log 命令可以查看历史记录，git log 命令显示从最近到最远的提交日志；如果嫌输出信息太多，看得眼花缭乱的，可以试试 git log --pretty=oneline 。我们可以看到当前版本以及之前的版本日志以及版本号。用 git log --graph 命令可以看到分支合并图。或者两个参数一起用：git log --graph --pretty=onelinegit log --graph --pretty=oneline --abbrev-commit首先，git 必须知道当前版本是哪个版本，在 git 中，用 HEAD 表示当前版本，上一个版本就是 HEAD^ ，上上一个版本就是 HEAD^^ ， 当然往上100个版本写100个 ^ 比较容易数不过来，所以写成 HEAD~100 。现在，我们要把当前版本回退到上一个版本，就可以使用 git reset 命令：git reset --hard HEAD^当你回退到了某个版本后，git log 只能显示此版本及之前的版本的日志，之后的版本日志就看不到了，但是，我们想恢复到之后教新的版本怎么办？Git 提供了一个命令 git reflog 用来记录你的每一次命令tag查看标签（用来标记标志性的稳定版本信息）发布一个版本时，我们通常先在版本库中打一个标签(tag)，这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。相比于 commit 的版本号(40位16进制)，标签号则要好使的多。所以，tag 就是一个让人容易记住的有意义的名字，它跟某个 commit 绑定在一起。git tag [tag name]如果没有标签名，则为查看所有标签，带标签名则为新建标签git tag &lt;tag name&gt; 就可以打一个新标签还可以创建带有说明的标签，用 -a 指定标签名，-m 指定说明文字。git tag -a &lt;tag name&gt; -m &lt;comment&gt; ：添加带注释的标签eg: git tag -a v1.2 -m &#39;version 1.2 released&#39;git tag -a &lt;tag name&gt; &lt;md5&gt; ：对某个版本打标签默认标签是打在最新提交的 commit 上的。有时候，如果忘了打标签，比如，现在已经是周五了，但应该在周一打的标签没有打，怎么办？方法是找到历史提交的 commit id，然后打上就可以了。git log --pretty=oneline --abbrev-commit比方说要对 add merge 这次提交打标签，它对应的 commit id 是 6224937，那么我们就可以使用命令：git tag v1.2 6224937再用命令 git log 查看标签可以用 git show &lt;tagname&gt; 查看标签信息如果标签打错了，也可以删除：git tag -d v1.2如果要推送某个标签到远程，使用命令 git push origin &lt;tagname&gt;eg: git push origin v1.2如果标签已经推送到远程，要删除远程标签就要麻烦一点，先从本地删除：git tag -d v1.2然后，从远程删除；删除命令也是 push ，但是格式如下：git push origin :ref/tags/v1.2git tag -l &#39;[expression]&#39;查看那符合正则表达式的stashgit stash备份当前的工作区的内容，从最近的一次提交中读取相关内容，让工作区保证和上次提交的内容一致。同时，将当前的工作区内容保存到 Git 栈中。git stash pop从 Git 栈中读取最近一次保存的内容，恢复工作区的相关内容。由于可能存在多个 stash 的内容，所以用栈来管理，pop 会从最近的一个 stash 中读取内容并恢复。git stash list显示 Git 栈中内的所有备份，可以利用这个列表来决定从哪个地方恢复。git stash clear : 清空 Git 栈。使用 git 的时候，我们往往使用 branch 解决任务切换问题，例如，我们往往会建一个自己的分支去修改和调试代码，如果别人或者自己发现原有的分支上有个不得不修改的 bug，我们往往会把完成一半的代码 commit 提交到本地仓库，然后切换分支去修改 bug，改好之后再切换回来。这样的话往往 log 上会有大量不必要的记录。其实如果我们不想提交完成一半或者不完善的代码，但是却不得不去修改一个紧急 bug，那么使用 git stash 就可以将你当前未提交到本地的代码推入到 git 的栈中，这时候你的工作区间和上一次提交的内容是完全一样的，所以你可以放心的修 bug，等到修完 bug，提交到服务器上后，再使用 git stash apply 将以前一般的工作应用回来。也许有的人会说，那我可不可以多次将未提交的代码压入到栈中？答案是可以的。当你多次使用 git stash 命令后，你的栈里将充满了未提交的代码，这时候你会对将哪个版本应用回来有些困惑， git stash list 命令可以将当前的 Git 栈信息打印出来，你只需要将找到对应的版本号，例如使用 git stash apply stash@{1} 就可以将你指定版本号为 stash@{1} 的工作取出来，当你将所有的栈都应用回来的时候，可以使用 git stash clear 来将栈清空。在这里顺便提下 git format-patch -n , n是具体某个数字， 例如 ‘git format-patch -1’ 这时便会根据log生成一个对应的补丁，如果 ‘git format-patch -2’ 那么便会生成 2 个补丁，当然前提是你的 log 上有至少有两个记录。看过上面的信息，就可以知道使用场合了：当前工作区内容已被修改，但是并未完成。这时 Boss 来了，说前面的分支上面有一个 bug，需要立即修复。可是我又不想提交目前的修改，因为修改没有完成。但是，不提交的话，又没有办法 checkout 到前面的分支。此时用 git stash 就相当于备份了工作区了。然后在 checkout 过去修改，就能够达到保存当前工作区，并及时恢复的作用。注意这里由于只 stash 了一次所以要使用 pop，具体你存放了多少。remote查看远程仓库名git remote -v查看远程仓库urlgit remote add &lt;basename&gt; &lt;url&gt;新增远程仓库git remote show &lt;basename&gt;查看远程仓库详细信息git remote rename &lt;old basename&gt; &lt;new basename&gt;重命名远程仓库commitgit commit -a -m &#39;xx&#39;暂存并提交branchgit branch查看本地仓库分支git branch -r查看远程分支情况git branch -a查看本地和远程的所有分支情况git branch -v查看本地仓库分支最后一次提交情况git branch -vv查看分支跟踪情况git branch &lt;branch name&gt;新建分支git branch -d &lt;branch name&gt;删除分支git branch -D &lt;branch name&gt;强制删除分支git branch [--merged | --no-merged]查看已合并|未合并的本地仓库分支git branch -u &lt;remote base&gt;/&lt;remote branch&gt;修改当前跟踪分支checkoutgit checkout -- [file] ：恢复文件git checkout -- demo.html 意思就是，把 demo.html 文件在工作区的修改全部撤销，这里有两种情况：一种是 demo.html 自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是 demo.html 已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加暂存区后的状态。总之，就是让这个文件回到最后一次 git commit 或 git add 时的状态。但是如果 git add 到暂存区了，在 commit 之前，想撤销：Git 同样告诉我们，用命令 git reset HEAD file 可以把暂存区的修改撤销掉(unstage)，重新放回工作区。git reset 命令既可以回退版本，也可以把暂存区的修改回退到工作区，当我们用 HEAD 时，表示最新的版本。再用 git status 查看一下，现在暂存区是干净的，工作区有修改：还记得如果丢弃工作区的修改吗？对的，使用：git checkout -- demo.htmlgit checkout 其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以”一键还原”。git checkout -b [branchname] [tagname]在特定的版本上创建一个新的分支并切换到此分支git checkout -b [local branch] [remote base]/[remote branch]将远程分支检出到本地分支，并追踪git checkout --track &lt;remote base&gt;/&lt;remote branch&gt;让当前分支跟踪远程分支rebasegit rebase [basebranch]变基是将一系列提交按照原有次序依次应用到另一分支上，而合并是把最终结果合在一起。常见问题git clonegit clone 支持多种协议，除了HTTP(s)以外，还支持SSH、Git、本地文件协议等。使用 https 除了速度慢意外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放 http 端口的公司内部就无法使用 ssh 协议而只能用 https。git clone &lt;版本库的网址&gt; &lt;本地目录名&gt;如果不写本地目录名，默认就是版本库的名字如何新建分支本地建立 branch 並立即切换到新分支git checkout -b &lt;branch_name&gt;下面的命令表示，在 origin/master 的基础上，创建一个分支。git checkout -b newBranch origin/master修改分支名称git branch -m &lt;new_name&gt;从远程仓库拉取代码到本地仓库，并建立跟踪关系git checkout -b &lt;本地新分支名&gt; &lt;对应的远程分支名&gt;如何在远程仓库新建一个分支新建一个本地分支，按照正常流程提交完代码后，推送到远程git push &lt;remote base&gt; &lt;local branch&gt;:&lt;remote branch&gt;比较文件git diff HEAD -- demo.html命令可以查看工作区的 demo.html 和版本库里面最新版本的区别。忽略某些文件默认方法是在当前项目目录下创建一个 .gitignore 文件，如果需要忽略的文件已经添加到版本库中，请先移除git rm --cached [file]不删除文件，只移除追踪。123cat .gitignore*.[oa]*~文件 .gitignore 的格式规范如下：所有空行或者以注释符号 # 开头的目录都会被 git 忽略可以使用标准的 glob 模式匹配匹配模式最后跟反斜杠（/）说明要忽略的目录要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反bug 分支git 提供了一个 stash 功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作。git stash修改完 bug 后，回到当前分支上继续干活，工作区是干净的，刚才的工作现场存到哪里去了？git stash list ：查看 stash 列表(stash 是一个栈的结构)git 把 stash 内容存在某个地方了，但是需要恢复一下，有两个办法：一是用 git stash apply 恢复，但是恢复后，stash 内容并不删除，你需要用 git stash drop 来删除；另一种方式是用 git stash pop ，恢复的同时把 stash 内容也删了；你可以多次 stash ，恢复的时候，先用 git stash list 查看，然后恢复指定的 stash，用命令：git stash apply stash@{0}配置文件配置 Git 的时候，加上 –global 是针对当前用户起作用的，如果不加，那只针对当前的仓库起作用。配置文件放哪了？每个仓库的 Git 配置文件都放在 .git/config 文件中，在这份配置文件中，别名就在 [alias] 后面，要删除别名，直接把对应的行删掉即可。查看配置git config -1设置git push 默认git config --global push.default current设置别名git config --global alias.&lt;name&gt; &lt;commend&gt;我的设置：git config --global alias.st statusgit config --global alias.cm &quot;commit -m&quot;git config --global alias.ph &quot;push origin &lt;local_repository&gt;:&lt;remote_repository&gt;&quot;保存用户名和密码对于http(s)协议，可以用下面命令临时缓存git config --global credential.helper cache开启linux缓存git config --global credential.helper wincred开启windows缓存对于 ssh 协议，可以用 ssh key，具体教程网上很多解决问题问题一git 中执行命令 add .报错：Unlink of file ‘templates/opms.exe’ failed.Should I try again?(y/n)因为这个文件正在被占用，所以不能添加到暂存区，而正好这个 .exe 文件，我们是不需要添加到版本管理工具的。所以我们选择 n 。问题二git 中生成 sshkey: ssh-keygen -t rsa -C &quot;youremail&quot;这个email并没有什么用所以我们使用ssh-keygen -t rsa来生成sshkey就可以了。然后git中的配置文件：git config --listgit config --global user.name &quot;yu&quot;git config --global user.email &quot;react.dong.yu@gmail.com&quot;这种配置将会对本地所有的git仓库有效。那么在 push 的时候，远程就知道这个push来自于哪个email.但有时候在公司的时候，有的仓库是公司的，有的仓库是自己github的。这个时候就可以不设置global的配置了，而是在自己的仓库中设置git config --local user.email &quot;react.dong.yu@gmail.com&quot;问题三使用 windows 的同学注意了，如果你在资源管理器里新建一个 .gitignore 文件，它会提示你必须输入文件名，但是在文本编辑器里“保存”或者“另存为”就可以把文件保存为 .gitignore 了。有些时候，你想添加一个文件到 git，但发现添加不了，原因是这个文件被 .gitignore 忽略了：git add App.class如果你确实想添加该文件，可以用 -f 制添加到 git：git add -f App.class或者你发现，可能是 .gitignore 写得有问题，需要找出来到底哪个规则写错了。可以用 git check-ignore 命令检查：git check-ignore -v App.class问题四为什么我把我生成的 ssh key 添加到了 github 中然后 也 remote 了 https://github.com/Neveryu/Xxx.git为什么提交的时候报错，或者提示 输入密码账号是为什么ssh key 是 ssh 协议的密钥，http 协议没权限问题五git怎样删除未监视的文件 untracked files?用 git clean12345678910111213# 删除 untracked filesgit clean -f # 连 untracked 的目录也一起删掉git clean -fd # 连 gitignore 的untrack 文件/目录也一起删掉 （慎用，一般这个是用来删掉编译出来的 .o之类的文件用的）git clean -xfd # 在用上述 git clean 前，墙裂建议加上 -n 参数来先看看会删掉哪些文件，防止重要文件被误删git clean -nxfdgit clean -nfgit clean -nfd我的常用命令12git branch -avvgit remote -v权威教程Pro Git 简体中文版","categories":[{"name":"综合","slug":"综合","permalink":"http://yoursite.com/categories/综合/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"如果深圳的夜晚没有风","slug":"essay-20160925","date":"2016-09-25T15:25:24.000Z","updated":"2018-10-20T07:46:51.000Z","comments":false,"path":"2016/09/25/essay-20160925/","link":"","permalink":"http://yoursite.com/2016/09/25/essay-20160925/","excerpt":"这座城市人很多，每天在路上都能看到好多拉着行李箱的人，不管他们是来到这座城市还是离开这座城市，至少他们都曾努力过。","text":"这座城市人很多，每天在路上都能看到好多拉着行李箱的人，不管他们是来到这座城市还是离开这座城市，至少他们都曾努力过。其实专业做博客的网站还是挺多的，比如说 CSDN 是吧，我之前也是在 CSDN 上面写一点东西，我的 CSDN 博客 http://blog.csdn.net/csdn_yudong 。但是 CSDN 毕竟是一个技术类的博客网站，如果写一点生活杂谈还是不太好，而且 CSDN 上面挂载的广告，以及页面风格，我不太喜欢，因为我喜欢【精于心，简于形】，这波 NexT 主题的广告是不是很硬。在我想做博客之前，一直到我开始着手做这个博客，其实花了好久时间。而在我着手开始做这个博客，到这个博客完成，其实并没有花多久。也说明了一个道理，就是一定要勇于尝试，关键是要动手。我想起了马士兵老师的一句话，一个新鲜的事物，你一定要先去用它，应用驱动学习。——我的建站日志有记录网站的建设过程。究其原因：第一个，我觉得是自己考虑的太多，没有太大的把握就不会轻易的去尝试，所以我在着手做这个之前花了很多的时间去查，域名空间这些的，而且这些出售这些的服务商也挺多的，我这个人有选择恐惧症。也是怪尴尬的，每次看到一个东西有好多选择的时候，我都去比较，去分析，头痛。。而且我想，域名空间搞好以后，还要自己写页面这些的。关键是还要设计博客风格，页面样式这些的，一想到这些，就感觉是一个不小的工程啊，后来才发现我多虑了。后来，无意中看到了 Hexo 的 Next 主题的一篇博客，风格样式挺不错的，感谢 Hexo 和 Next 的文档写的如此的棒，以至于每一个人都能根据教程搭建一个自己的博客。这一切是如此的美好。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/categories/随笔/"}],"tags":[{"name":"essay","slug":"essay","permalink":"http://yoursite.com/tags/essay/"}]}]}